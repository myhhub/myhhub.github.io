<!DOCTYPE HTML>
<html lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
    <!--Setting-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta http-equiv="Cache-Control" content="no-siteapp">
    <meta http-equiv="Cache-Control" content="no-transform">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="apple-mobile-web-app-capable" content="来唧唧歪歪(Ljjyy.com) - 多读书多实践，勤思考善领悟">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    <meta name="browsermode" content="application">
    <meta name="screen-orientation" content="portrait">
    <meta name="theme-version" content="1.2.3">
    <meta name="root" content="/">
    <link rel="dns-prefetch" href="https://www.ljjyy.com">
    <!--SEO-->

    <meta name="keywords" content="hadoop,hdfs">


    <meta name="description" content="HDFS常用Shell命令HDFS文件操作&#160; &#160; &#160; &#160;HDFS是一种文件系统,专为MapReduce这类框架下的大规模分布式数据处理而设计,你可以把一个...">



<meta name="robots" content="all">
<meta name="google" content="all">
<meta name="googlebot" content="all">
<meta name="verify" content="all">

    <!--Title-->


<title>大数据hadoop之 十一.组件HDFS | 来唧唧歪歪(Ljjyy.com) - 多读书多实践，勤思考善领悟</title>


    <link rel="alternate" href="/atom.xml" title="来唧唧歪歪(Ljjyy.com) - 多读书多实践，勤思考善领悟" type="application/atom+xml">


    <link rel="icon" href="/favicon.ico">

    



<link rel="stylesheet" href="/css/bootstrap.min.css?rev=3.3.7">
<link rel="stylesheet" href="/css/font-awesome.min.css?rev=4.5.0">
<link rel="stylesheet" href="/css/style.css?rev=@@hash">




    
	<div class="hide">
        <script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script>
        <script>LA.init({id: "JgbNOaw1xxsmUUsQ",ck: "JgbNOaw1xxsmUUsQ"})</script>
	</div>






    
    <meta name="baidu-site-verification" content="dTHILoORpx">


    <script>
        (function(){
            var bp = document.createElement('script');
            var curProtocol = window.location.protocol.split(':')[0];
            if (curProtocol === 'https') {
                bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
            }
            else {
                bp.src = 'http://push.zhanzhang.baidu.com/push.js';
            }
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(bp, s);
        })();
    </script>

</head>

</html>
<!--[if lte IE 8]>
<style>
    html{ font-size: 1em }
</style>
<![endif]-->
<!--[if lte IE 9]>
<div style="ie">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div>
<![endif]-->

<body>
    <header class="main-header"  >
    <div class="main-header-box">
        <!--a class="header-avatar" href="/" title='Ljjyy.com'>
            <img src="/img/avatar.jpg" alt="logo头像" class="img-responsive center-block">
        </a-->
        <div class="branding">
            
                <h2> 多读书多实践，勤思考善领悟 </h2>
            
    	  </div>
    </div>
</header>
    <nav class="main-navigation">
    <div class="container">

        <div class="row">
            <div class="col-sm-12">
                <div class="navbar-header"><span class="nav-toggle-button collapsed pull-right" data-toggle="collapse" data-target="#main-menu" id="mnav">
                    <span class="sr-only"></span>
                        <i class="fa fa-bars"></i>
                    </span>
                    <a class="web-logo"  href="/" title='Ljjyy.com'></a>
                    <!--a class="navbar-brand" href="https://www.ljjyy.com">来唧唧歪歪(Ljjyy.com) - 多读书多实践，勤思考善领悟</a-->
                </div>
                <div class="collapse navbar-collapse" id="main-menu" style="">
                    <ul class="menu">
                        
                            <li role="presentation" class="text-center">
                                <a href="/"><i class="fa "></i>首页</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/cloud/"><i class="fa "></i>云计算</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/front/"><i class="fa "></i>前端</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/back/"><i class="fa "></i>后端</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/devops/"><i class="fa "></i>运维</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/crack/"><i class="fa "></i>破解</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/penetration/"><i class="fa "></i>渗透</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/tool/"><i class="fa "></i>工具</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/other/"><i class="fa "></i>其他</a>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </div>
    </div>
</nav>
    <section class="content-wrap">
        <div class="container">
            <div class="row">
                <main class="col-md-8 main-content m-post">
                    <p id="process"></p>
<article class="post">
    <div class="post-head">
        <h1 id="大数据hadoop之 十一.组件HDFS">
            
	            大数据hadoop之 十一.组件HDFS
            
        </h1>
        <div class="post-meta">
    
        <span class="categories-meta fa-wrap">
            <i class="fa fa-folder-open-o"></i>
            <a class="category-link" href="/categories/cloud/">云计算</a>
        </span>
    

    
        <span class="fa-wrap">
            <i class="fa fa-tags"></i>
            <span class="tags-meta">
                
                    <a class="tag-link" href="/tags/hadoop/">hadoop</a> <a class="tag-link" href="/tags/hdfs/">hdfs</a>
                
            </span>
        </span>
    

    
        
        <span class="fa-wrap">
            <i class="fa fa-clock-o"></i>
            <span class="date-meta">2019/06/10</span>
        </span>
        
    
</div>
            
            
            <p class="fa fa-exclamation-triangle warning">
                本文于<strong>1397</strong>天之前发表，文中内容可能已经过时。
            </p>
        
    </div>
    
    <div class="post-body post-content">
        <h1 id="HDFS常用Shell命令"><a href="#HDFS常用Shell命令" class="headerlink" title="HDFS常用Shell命令"></a>HDFS常用Shell命令</h1><h2 id="HDFS文件操作"><a href="#HDFS文件操作" class="headerlink" title="HDFS文件操作"></a>HDFS文件操作</h2><p>&#160; &#160; &#160; &#160;HDFS是一种文件系统,专为MapReduce这类框架下的大规模分布式数据处理而设计,你可以把一个大数据集(比如说100TB)在HDFS中存储为单个文件,而大多数其他的文件系统无力实现这一点.<br>&#160; &#160; &#160; &#160;HDFS并不是一个天生的UNIX文件系统,不支持像ls和cp这种标准的UNIX文件命令,也不支持如fopen()和fread()这样的标准文件读写操作.另一方面,Hadoop确也提供了一套与Linux文件命令类似的命令行工具.</p>
<h3 id="基本文件fs命令"><a href="#基本文件fs命令" class="headerlink" title="基本文件fs命令"></a>基本文件fs命令</h3><p>Hadoop的文件命令采取的形式为</p>
<p><code>hadoop fs -cmd &lt;args&gt;</code>  </p>
<p>基中cmd是具体的文件命令,而<code>&lt;args&gt;</code>是一组数据可变的参数.<code>cmd</code>的命名通常与unix对应的命令名相同.例如,文件形表命令为  </p>
<figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hadoop <span class="built_in">fs</span> -ls  </span><br><span class="line"></span><br><span class="line">hadoop <span class="built_in">fs</span> -<span class="built_in">mkdir</span> /user/chuck</span><br><span class="line">hadoop <span class="built_in">fs</span> -ls /</span><br><span class="line">hadoop <span class="built_in">fs</span> -put example.txt /user/chuck</span><br><span class="line">hadoop <span class="built_in">fs</span> -ls</span><br><span class="line">hadoop <span class="built_in">fs</span> -get example.txt .</span><br><span class="line">hadoop <span class="built_in">fs</span> -cat example.txt</span><br><span class="line">hadoop <span class="built_in">fs</span> -rm  example.txt</span><br></pre></td></tr></table></figure>
<h4 id="1-显示当前目录结构"><a href="#1-显示当前目录结构" class="headerlink" title="1. 显示当前目录结构"></a><strong>1. 显示当前目录结构</strong></h4><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># 显示当前目录结构</span></span><br><span class="line">hadoop fs -ls  &lt;path&gt;</span><br><span class="line"><span class="meta"># 递归显示当前目录结构</span></span><br><span class="line">hadoop fs -ls  -R  &lt;path&gt;</span><br><span class="line"><span class="meta"># 显示根目录下内容</span></span><br><span class="line">hadoop fs -ls  /</span><br></pre></td></tr></table></figure>
<h4 id="2-创建目录"><a href="#2-创建目录" class="headerlink" title="2. 创建目录"></a><strong>2. 创建目录</strong></h4><figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 创建目录</span><br><span class="line">hadoop <span class="built_in">fs</span> -<span class="built_in">mkdir</span>  &lt;<span class="built_in">path</span>&gt; </span><br><span class="line"># 递归创建目录</span><br><span class="line">hadoop <span class="built_in">fs</span> -<span class="built_in">mkdir</span> -p  &lt;<span class="built_in">path</span>&gt;</span><br></pre></td></tr></table></figure>
<p><strong>3. 删除操作</strong></p>
<figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 删除文件</span><br><span class="line">hadoop <span class="built_in">fs</span> -rm  &lt;<span class="built_in">path</span>&gt;</span><br><span class="line"># 递归删除目录和文件</span><br><span class="line">hadoop <span class="built_in">fs</span> -rm -R  &lt;<span class="built_in">path</span>&gt;</span><br></pre></td></tr></table></figure>
<h4 id="4-从本地加载文件到HDFS"><a href="#4-从本地加载文件到HDFS" class="headerlink" title="4. 从本地加载文件到HDFS"></a><strong>4. 从本地加载文件到HDFS</strong></h4><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 二选一执行即可</span><br><span class="line">hadoop fs -put  <span class="string">[localsrc]</span> <span class="string">[dst]</span> </span><br><span class="line">hadoop fs - copyFromLocal <span class="string">[localsrc]</span> <span class="string">[dst]</span></span><br></pre></td></tr></table></figure>
<h4 id="5-从HDFS导出文件到本地"><a href="#5-从HDFS导出文件到本地" class="headerlink" title="5. 从HDFS导出文件到本地"></a><strong>5. 从HDFS导出文件到本地</strong></h4><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 二选一执行即可</span><br><span class="line">hadoop fs -get  <span class="string">[dst]</span> <span class="string">[localsrc]</span> </span><br><span class="line">hadoop fs -copyToLocal <span class="string">[dst]</span> <span class="string">[localsrc]</span></span><br></pre></td></tr></table></figure>
<p><strong>6. 查看文件内容</strong></p>
<figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 二选一执行即可</span><br><span class="line">hadoop <span class="built_in">fs</span> -text  &lt;<span class="built_in">path</span>&gt; </span><br><span class="line">hadoop <span class="built_in">fs</span> -cat  &lt;<span class="built_in">path</span>&gt;</span><br></pre></td></tr></table></figure>
<h4 id="7-显示文件的最后一千字节"><a href="#7-显示文件的最后一千字节" class="headerlink" title="7. 显示文件的最后一千字节"></a><strong>7. 显示文件的最后一千字节</strong></h4><figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop <span class="built_in">fs</span> -tail  &lt;<span class="built_in">path</span>&gt; </span><br><span class="line"># 和Linux下一样，会持续监听文件内容变化 并显示文件的最后一千字节</span><br><span class="line">hadoop <span class="built_in">fs</span> -tail -f  &lt;<span class="built_in">path</span>&gt;</span><br></pre></td></tr></table></figure>
<h4 id="8-拷贝文件"><a href="#8-拷贝文件" class="headerlink" title="8. 拷贝文件"></a><strong>8. 拷贝文件</strong></h4><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cp <span class="string">[src]</span> <span class="string">[dst]</span></span><br></pre></td></tr></table></figure>
<h4 id="9-移动文件"><a href="#9-移动文件" class="headerlink" title="9. 移动文件"></a><strong>9. 移动文件</strong></h4><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mv <span class="string">[src]</span> <span class="string">[dst]</span></span><br></pre></td></tr></table></figure>
<h4 id="10-统计当前目录下各文件大小"><a href="#10-统计当前目录下各文件大小" class="headerlink" title="10. 统计当前目录下各文件大小"></a><strong>10. 统计当前目录下各文件大小</strong></h4><ul>
<li>默认单位字节</li>
<li>-s : 显示所有文件大小总和，</li>
<li>-h : 将以更友好的方式显示文件大小（例如64.0m而不是67108864）</li>
</ul>
<figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop <span class="built_in">fs</span> -du  &lt;<span class="built_in">path</span>&gt;</span><br></pre></td></tr></table></figure>
<h4 id="11-合并下载多个文件"><a href="#11-合并下载多个文件" class="headerlink" title="11. 合并下载多个文件"></a><strong>11. 合并下载多个文件</strong></h4><ul>
<li>-nl 在每个文件的末尾添加换行符（LF）</li>
<li>-skip-empty-file 跳过空文件</li>
</ul>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -getmerge</span><br><span class="line"><span class="comment"># 示例 将HDFS上的hbase-policy.xml和hbase-site.xml文件合并后下载到本地的/usr/test.xml</span></span><br><span class="line">hadoop fs -getmerge -nl  /test/hbase-policy.<span class="keyword">xml</span> <span class="title">/test</span>/hbase-site.<span class="keyword">xml</span> <span class="title">/usr</span>/test.xml</span><br></pre></td></tr></table></figure>
<h4 id="12-统计文件系统的可用空间信息"><a href="#12-统计文件系统的可用空间信息" class="headerlink" title="12. 统计文件系统的可用空间信息"></a><strong>12. 统计文件系统的可用空间信息</strong></h4><figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop <span class="built_in">fs</span> -df -h /</span><br></pre></td></tr></table></figure>
<h4 id="13-更改文件复制因子"><a href="#13-更改文件复制因子" class="headerlink" title="13. 更改文件复制因子"></a><strong>13. 更改文件复制因子</strong></h4><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -setrep <span class="string">[-R]</span> <span class="string">[-w]</span> &lt;numReplicas&gt; &lt;path&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>更改文件的复制因子。如果path是目录，则更改其下所有文件的复制因子</li>
<li>-w : 请求命令是否等待复制完成</li>
</ul>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例</span></span><br><span class="line">hadoop fs -setrep -w <span class="number">3</span> <span class="regexp">/user/</span>hadoop<span class="regexp">/dir1</span></span><br></pre></td></tr></table></figure>
<h4 id="14-权限控制"><a href="#14-权限控制" class="headerlink" title="14. 权限控制"></a><strong>14. 权限控制</strong></h4><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># 权限控制和Linux上使用方式一致</span></span><br><span class="line"><span class="section"># 变更文件或目录的所属群组。 用户必须是文件的所有者或超级用户。</span></span><br><span class="line">hadoop fs -chgrp [-R] GROUP URI [URI ...]</span><br><span class="line"><span class="section"># 修改文件或目录的访问权限  用户必须是文件的所有者或超级用户。</span></span><br><span class="line">hadoop fs -chmod [-R] <span class="xml"><span class="tag">&lt;<span class="name">MODE[,MODE]...</span> | <span class="attr">OCTALMODE</span>&gt;</span></span> URI [URI ...]</span><br><span class="line"><span class="section"># 修改文件的拥有者  用户必须是超级用户。</span></span><br><span class="line">hadoop fs -chown [<span class="string">-R</span>] [<span class="string">OWNER</span>][<span class="symbol">:[GROUP</span>]] URI [URI ]</span><br></pre></td></tr></table></figure>
<h4 id="15-文件检测"><a href="#15-文件检测" class="headerlink" title="15. 文件检测"></a><strong>15. 文件检测</strong></h4><figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop <span class="built_in">fs</span> -<span class="keyword">test</span> - [defsz]  URI</span><br></pre></td></tr></table></figure>
<p>可选选项：</p>
<ul>
<li>-d：如果路径是目录，返回0。</li>
<li>-e：如果路径存在，则返回0。</li>
<li>-f：如果路径是文件，则返回0。</li>
<li>-s：如果路径不为空，则返回0。</li>
<li>-r：如果路径存在且授予读权限，则返回0。</li>
<li>-w：如果路径存在且授予写入权限，则返回0。</li>
<li>-z：如果文件长度为零，则返回0。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例</span></span><br><span class="line">hadoop fs -<span class="built_in">test</span> -e filename</span><br></pre></td></tr></table></figure>
<h3 id="cat命令"><a href="#cat命令" class="headerlink" title="cat命令"></a>cat命令</h3><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">将路径指定文件的内容输出到stdout</span><br><span class="line">hadoop<span class="variable">@Master</span><span class="symbol">:~</span><span class="variable">$ </span>hadoop dfs -cat input/core-site.xml</span><br></pre></td></tr></table></figure>
<h3 id="chgrp命令"><a href="#chgrp命令" class="headerlink" title="chgrp命令"></a>chgrp命令</h3><figure class="highlight erlang"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">改变文件所属组.使用-R将使改变在目录结构下递归进行.命令的使用者必须是文件的所有者或者超级用户.</span><br></pre></td></tr></table></figure>
<h3 id="chmod命令"><a href="#chmod命令" class="headerlink" title="chmod命令"></a>chmod命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">修改文件权限</span><br></pre></td></tr></table></figure>
<h3 id="chown命令"><a href="#chown命令" class="headerlink" title="chown命令"></a>chown命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">改变文件的拥有者</span><br></pre></td></tr></table></figure>
<h3 id="cp命令"><a href="#cp命令" class="headerlink" title="cp命令"></a>cp命令</h3><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">将文件从源路径复制到目标路径,这个命令允许有多个源路径,此时目标路径必须是一个目录</span><br><span class="line">hadoop@<span class="literal">Master</span>:/usr/local/hadoop/etc/hadoop$ hdfs dfs -cp input/* output/</span><br><span class="line">hadoop@<span class="literal">Master</span>:/usr/local/hadoop/etc/hadoop$ hdfs dfs -ls output</span><br><span class="line">Found <span class="number">11</span> items</span><br><span class="line">-rw-r--r--   <span class="number">1</span> hadoop supergroup          <span class="number">0</span> <span class="number">2016</span>-<span class="number">01</span>-<span class="number">25</span> <span class="number">22</span>:<span class="number">14</span> output/_SUCCESS</span><br><span class="line">-rw-r--r--   <span class="number">1</span> hadoop supergroup       <span class="number">4436</span> <span class="number">2016</span>-<span class="number">01</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">16</span> output/capacity-scheduler.xml</span><br><span class="line">-rw-r--r--   <span class="number">1</span> hadoop supergroup       <span class="number">1072</span> <span class="number">2016</span>-<span class="number">01</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">16</span> output/core-site.xml</span><br><span class="line">-rw-r--r--   <span class="number">1</span> hadoop supergroup       <span class="number">9683</span> <span class="number">2016</span>-<span class="number">01</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">16</span> output/hadoop-policy.xml</span><br><span class="line">-rw-r--r--   <span class="number">1</span> hadoop supergroup       <span class="number">1257</span> <span class="number">2016</span>-<span class="number">01</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">16</span> output/hdfs-site.xml</span><br><span class="line">-rw-r--r--   <span class="number">1</span> hadoop supergroup        <span class="number">620</span> <span class="number">2016</span>-<span class="number">01</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">16</span> output/httpfs-site.xml</span><br><span class="line">-rw-r--r--   <span class="number">1</span> hadoop supergroup       <span class="number">3523</span> <span class="number">2016</span>-<span class="number">01</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">16</span> output/kms-acls.xml</span><br><span class="line">-rw-r--r--   <span class="number">1</span> hadoop supergroup       <span class="number">5511</span> <span class="number">2016</span>-<span class="number">01</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">16</span> output/kms-site.xml</span><br><span class="line">-rw-r--r--   <span class="number">1</span> hadoop supergroup       <span class="number">1103</span> <span class="number">2016</span>-<span class="number">01</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">16</span> output/mapred-site.xml</span><br><span class="line">-rw-r--r--   <span class="number">1</span> hadoop supergroup        <span class="number">107</span> <span class="number">2016</span>-<span class="number">01</span>-<span class="number">25</span> <span class="number">22</span>:<span class="number">14</span> output/part-r-<span class="number">00000</span></span><br><span class="line">-rw-r--r--   <span class="number">1</span> hadoop supergroup        <span class="number">924</span> <span class="number">2016</span>-<span class="number">01</span>-<span class="number">27</span> <span class="number">19</span>:<span class="number">16</span> output/yarn-site.xml</span><br></pre></td></tr></table></figure>
<h3 id="du命令"><a href="#du命令" class="headerlink" title="du命令"></a>du命令</h3><p>显示目录中所有文件的大小,或者当只指定一个文件时,显示此文件的大小.<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop<span class="variable">@Master</span><span class="symbol">:/usr/local/hadoop/etc/hadoop</span><span class="variable">$ </span>hadoop fs -du input/core-site.xml</span><br></pre></td></tr></table></figure></p>
<h3 id="expunge命令"><a href="#expunge命令" class="headerlink" title="expunge命令"></a>expunge命令</h3><p>清空回收站<br>除了文件权限之外,还有一个保护机制可以防止在HDFS上意外删除文件,这就是回收站,默认情况下该功能是被禁用.当它启用后,用于删除的命令行不会立即删除文件.<br>相反它们会暂时的把文件移动到用户工作目录下的.Trash文件夹下.若要启用回收站功能并设置清空回收站的时间延迟,可能通过设置core-site.xml的fs.trash.interval属性(以分钟为单位).<br>例如如果你希望用户有24个小时的时间来还原已删除的文件,就应该在core-site.xml中设置.<br>如果将该值设置为0,则将禁用回收站的功能</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>1440<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h3 id="get命令"><a href="#get命令" class="headerlink" title="get命令"></a>get命令</h3><p>复制文件到本地文件系统.<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -get input/hadoop<span class="selector-class">.tar</span><span class="selector-class">.gz</span> ~/</span><br></pre></td></tr></table></figure></p>
<h3 id="lsr命令"><a href="#lsr命令" class="headerlink" title="lsr命令"></a>lsr命令</h3><p>ls命令的递归版本,类似于Unix中的ls -R</p>
<p>###mkdir命令</p>
<p>接受路径指定的uri作为参数,创建这些目录,其行为类似于Unix的mkdir -p,它会创建路径中的各级父目录.<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir <span class="regexp">/user/</span>hadoop<span class="regexp">/dir1 /u</span>ser<span class="regexp">/hadoop/</span>dir2</span><br></pre></td></tr></table></figure></p>
<h3 id="mv命令"><a href="#mv命令" class="headerlink" title="mv命令"></a>mv命令</h3><p>将文件从源路径移动到目标路径<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mv <span class="regexp">/user/</span>hadoop<span class="regexp">/file1 /u</span>ser<span class="regexp">/hadoop/</span>file2</span><br></pre></td></tr></table></figure></p>
<h3 id="put命令"><a href="#put命令" class="headerlink" title="put命令"></a>put命令</h3><p>从本地文件系统中复制单个或者多个源路径到目标文件系统.也支持从标准输入中读取输入写入目标文件系统.<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put /tmp/*.<span class="keyword">xml</span> <span class="title">/user</span>/hadoop/</span><br></pre></td></tr></table></figure></p>
<h3 id="rmr命令"><a href="#rmr命令" class="headerlink" title="rmr命令"></a>rmr命令</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -rmr <span class="regexp">/user/</span>hadoop<span class="regexp">/chu888chu888</span></span><br></pre></td></tr></table></figure>
<h3 id="job命令"><a href="#job命令" class="headerlink" title="job命令"></a>job命令</h3><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">* Job操作</span><br><span class="line">* 提交MapReduce Job, Hadoop所有的MapReduce Job都是一个jar包</span><br><span class="line">* $ hadoop jar &lt;local-jar-file&gt; &lt;java-<span class="class"><span class="keyword">class</span>&gt; &lt;hdfs-<span class="title">input</span>-<span class="title">file</span>&gt; &lt;hdfs-<span class="title">output</span>-<span class="title">dir</span>&gt;</span></span><br><span class="line">* $ hadoop jar sandbox-mapred-<span class="number">0</span>.<span class="number">0</span>.<span class="number">20</span>.jar sandbox.mapred.WordCountJob /user/cl/input.dat /user/cl/outputdir</span><br><span class="line">*</span><br><span class="line">* 杀死某个正在运行的Job</span><br><span class="line">* 假设Job_Id为：job_201207121738_0001</span><br><span class="line">* $ hadoop job -kill job_201207121738_0001</span><br></pre></td></tr></table></figure>
<h3 id="系统体检"><a href="#系统体检" class="headerlink" title="系统体检"></a>系统体检</h3><p>Hadoop提供的文件系统检查工具叫做fsck,如参数为文件路径时,它会递归检查该路径下所有文件的健康状态,如果参数为/,它就会检查整个文件系统,如下输出一个例子.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">hadoop@Master:~$ hadoop fsck /</span><br><span class="line">DEPRECATED: <span class="keyword">Use</span> <span class="keyword">of</span> this script <span class="keyword">to</span> <span class="keyword">execute</span> hdfs command <span class="keyword">is</span> deprecated.</span><br><span class="line">Instead <span class="keyword">use</span> the hdfs command <span class="keyword">for</span> it.</span><br><span class="line"></span><br><span class="line"><span class="number">16</span>/<span class="number">01</span>/<span class="number">27</span> <span class="number">22</span>:<span class="number">55</span>:<span class="number">14</span> WARN util.NativeCodeLoader: Unable <span class="keyword">to</span> <span class="keyword">load</span> <span class="keyword">native</span>-hadoop <span class="keyword">library</span> <span class="keyword">for</span> your platform... <span class="keyword">using</span> builtin-<span class="keyword">java</span> classes <span class="keyword">where</span> applicable</span><br><span class="line">Connecting <span class="keyword">to</span> namenode via <span class="keyword">http</span>://<span class="keyword">Master</span>:<span class="number">50070</span></span><br><span class="line">FSCK started <span class="keyword">by</span> hadoop (auth:SIMPLE) <span class="keyword">from</span> /<span class="number">192.168</span><span class="number">.1</span><span class="number">.80</span> <span class="keyword">for</span> <span class="keyword">path</span> / <span class="keyword">at</span> Wed Jan <span class="number">27</span> <span class="number">22</span>:<span class="number">55</span>:<span class="number">15</span> CST <span class="number">2016</span></span><br><span class="line">.....................Status: HEALTHY</span><br><span class="line"> Total <span class="keyword">size</span>:	<span class="number">878899</span> B</span><br><span class="line"> Total dirs:	<span class="number">21</span></span><br><span class="line"> Total files:	<span class="number">21</span></span><br><span class="line"> Total symlinks:		<span class="number">0</span></span><br><span class="line"> Total blocks (validated):	<span class="number">20</span> (avg. <span class="keyword">block</span> <span class="keyword">size</span> <span class="number">43944</span> B)</span><br><span class="line"> Minimally replicated blocks:	<span class="number">20</span> (<span class="number">100.0</span> %)</span><br><span class="line"> <span class="keyword">Over</span>-replicated blocks:	<span class="number">0</span> (<span class="number">0.0</span> %)</span><br><span class="line"> <span class="keyword">Under</span>-replicated blocks:	<span class="number">0</span> (<span class="number">0.0</span> %)</span><br><span class="line"> Mis-replicated blocks:		<span class="number">0</span> (<span class="number">0.0</span> %)</span><br><span class="line"> <span class="keyword">Default</span> <span class="keyword">replication</span> factor:	<span class="number">1</span></span><br><span class="line"> Average <span class="keyword">block</span> <span class="keyword">replication</span>:	<span class="number">1.0</span></span><br><span class="line"> Corrupt blocks:		<span class="number">0</span></span><br><span class="line"> <span class="keyword">Missing</span> replicas:		<span class="number">0</span> (<span class="number">0.0</span> %)</span><br><span class="line"> <span class="built_in">Number</span> <span class="keyword">of</span> <span class="keyword">data</span>-nodes:		<span class="number">2</span></span><br><span class="line"> <span class="built_in">Number</span> <span class="keyword">of</span> racks:		<span class="number">1</span></span><br><span class="line">FSCK ended <span class="keyword">at</span> Wed Jan <span class="number">27</span> <span class="number">22</span>:<span class="number">55</span>:<span class="number">15</span> CST <span class="number">2016</span> <span class="keyword">in</span> <span class="number">32</span> milliseconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The filesystem <span class="keyword">under</span> <span class="keyword">path</span> <span class="string">'/'</span> <span class="keyword">is</span> HEALTHY</span><br></pre></td></tr></table></figure>
<h1 id="HDFS-Java-API的使用"><a href="#HDFS-Java-API的使用" class="headerlink" title="HDFS Java API的使用"></a>HDFS Java API的使用</h1><h2 id="一、-简介"><a href="#一、-简介" class="headerlink" title="一、 简介"></a>一、 简介</h2><p>想要使用HDFS API，需要导入依赖<code>hadoop-client</code>。如果是CDH版本的Hadoop，还需要额外指明其仓库地址：</p>
<figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span></span><br><span class="line"><span class="xml">         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"</span></span><br><span class="line"><span class="xml">         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 </span></span><br><span class="line"><span class="xml">         http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt;</span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.heibaiying<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hdfs-java-api<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">hadoop.version</span>&gt;</span>2.6.0-cdh5.15.2<span class="tag">&lt;/<span class="name">hadoop.version</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="comment">&lt;!---配置CDH仓库地址--&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="comment">&lt;!--Hadoop-client--&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$</span><span class="template-variable">&#123;hadoop.version&#125;</span><span class="xml"><span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.12<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span></span><br><span class="line"><span class="xml">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span></span><br><span class="line"><span class="xml">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span></span><br><span class="line"><span class="xml">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span></span><br><span class="line"><span class="xml"></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span></span><br></pre></td></tr></table></figure>
<h2 id="二、API的使用"><a href="#二、API的使用" class="headerlink" title="二、API的使用"></a>二、API的使用</h2><h3 id="2-1-FileSystem"><a href="#2-1-FileSystem" class="headerlink" title="2.1 FileSystem"></a>2.1 FileSystem</h3><p>FileSystem是所有HDFS操作的主入口。由于之后的每个单元测试都需要用到它，这里使用<code>@Before</code>注解进行标注。</p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String HDFS_PATH = <span class="string">"hdfs://192.168.0.106:8020"</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String HDFS_USER = <span class="string">"root"</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> FileSystem fileSystem;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">prepare</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">// 这里我启动的是单节点的Hadoop,所以副本系数设置为1,默认值为3</span></span><br><span class="line">        configuration.set(<span class="string">"dfs.replication"</span>, <span class="string">"1"</span>);</span><br><span class="line">        fileSystem = FileSystem.get(<span class="keyword">new</span> URI(HDFS_PATH), configuration, HDFS_USER);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (URISyntaxException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@After</span></span><br><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">destroy</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    fileSystem = <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-2-创建目录"><a href="#2-2-创建目录" class="headerlink" title="2.2 创建目录"></a>2.2 创建目录</h3><p>支持递归创建目录：</p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">mkDir</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    fileSystem.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test0/"</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-3-创建指定权限的目录"><a href="#2-3-创建指定权限的目录" class="headerlink" title="2.3 创建指定权限的目录"></a>2.3 创建指定权限的目录</h3><p><code>FsPermission(FsAction u, FsAction g, FsAction o)</code> 的三个参数分别对应：创建者权限，同组其他用户权限，其他用户权限，权限值定义在<code>FsAction</code>枚举类中。</p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">mkDirWithPermission</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    fileSystem.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test1/"</span>),</span><br><span class="line">            <span class="keyword">new</span> FsPermission(FsAction.READ_WRITE, FsAction.READ, FsAction.READ));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-4-创建文件，并写入内容"><a href="#2-4-创建文件，并写入内容" class="headerlink" title="2.4 创建文件，并写入内容"></a>2.4 创建文件，并写入内容</h3><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">create</span>(<span class="params"></span>) throws Exception</span> &#123;</span><br><span class="line">    <span class="comment">// 如果文件存在，默认会覆盖, 可以通过第二个参数进行控制。第三个参数可以控制使用缓冲区的大小</span></span><br><span class="line">    FSDataOutputStream <span class="keyword">out</span> = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/a.txt"</span>),</span><br><span class="line">                                               <span class="literal">true</span>, <span class="number">4096</span>);</span><br><span class="line">    <span class="keyword">out</span>.write(<span class="string">"hello hadoop!"</span>.getBytes());</span><br><span class="line">    <span class="keyword">out</span>.write(<span class="string">"hello spark!"</span>.getBytes());</span><br><span class="line">    <span class="keyword">out</span>.write(<span class="string">"hello flink!"</span>.getBytes());</span><br><span class="line">    <span class="comment">// 强制将缓冲区中内容刷出</span></span><br><span class="line">    <span class="keyword">out</span>.flush();</span><br><span class="line">    <span class="keyword">out</span>.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-5-判断文件是否存在"><a href="#2-5-判断文件是否存在" class="headerlink" title="2.5 判断文件是否存在"></a>2.5 判断文件是否存在</h3><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> exist() throws Exception &#123;</span><br><span class="line">    <span class="keyword">boolean</span> <span class="built_in">exists</span> = fileSystem.<span class="built_in">exists</span>(<span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/a.txt"</span>));</span><br><span class="line">    System.out.<span class="built_in">println</span>(<span class="built_in">exists</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-6-查看文件内容"><a href="#2-6-查看文件内容" class="headerlink" title="2.6 查看文件内容"></a>2.6 查看文件内容</h3><p>查看小文本文件的内容，直接转换成字符串后输出：</p>
<figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> readToString() <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    FSDataInputStream inputStream = fileSystem.<span class="built_in">open</span>(<span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/a.txt"</span>));</span><br><span class="line">    <span class="keyword">String</span> context = inputStreamToString(inputStream, <span class="string">"utf-8"</span>);</span><br><span class="line">    System.out.<span class="built_in">println</span>(context);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>inputStreamToString</code>是一个自定义方法，代码如下：</p>
<figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 把输入流转换为指定编码的字符</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param inputStream 输入流</span></span><br><span class="line"><span class="comment"> * @param encode      指定编码类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">String</span> inputStreamToString(InputStream inputStream, <span class="keyword">String</span> encode) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (encode == <span class="keyword">null</span> || (<span class="string">""</span>.equals(encode))) &#123;</span><br><span class="line">            encode = <span class="string">"utf-8"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">BufferedReader</span> reader = <span class="keyword">new</span> <span class="keyword">BufferedReader</span>(<span class="keyword">new</span> InputStreamReader(inputStream, encode));</span><br><span class="line">        StringBuilder builder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        <span class="keyword">String</span> <span class="built_in">str</span> = <span class="string">""</span>;</span><br><span class="line">        <span class="keyword">while</span> ((<span class="built_in">str</span> = reader.readLine()) != <span class="keyword">null</span>) &#123;</span><br><span class="line">            builder.<span class="built_in">append</span>(<span class="built_in">str</span>).<span class="built_in">append</span>(<span class="string">"\n"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> builder.toString();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-7-文件重命名"><a href="#2-7-文件重命名" class="headerlink" title="2.7 文件重命名"></a>2.7 文件重命名</h3><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line"><span class="keyword">public</span> void rename() throws Exception &#123;</span><br><span class="line">    Path oldPath = <span class="keyword">new</span> <span class="type">Path</span>(<span class="string">"/hdfs-api/test/a.txt"</span>);</span><br><span class="line">    Path <span class="keyword">new</span><span class="type">Path</span> = <span class="keyword">new</span> <span class="type">Path</span>(<span class="string">"/hdfs-api/test/b.txt"</span>);</span><br><span class="line">    boolean result = fileSystem.rename(oldPath, <span class="keyword">new</span><span class="type">Path</span>);</span><br><span class="line">    System.out.println(result);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-8-删除目录或文件"><a href="#2-8-删除目录或文件" class="headerlink" title="2.8 删除目录或文件"></a>2.8 删除目录或文件</h3><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="keyword">delete</span>() <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     *  第二个参数代表是否递归删除</span></span><br><span class="line"><span class="comment">     *    +  如果path是一个目录且递归删除为true, 则删除该目录及其中所有文件;</span></span><br><span class="line"><span class="comment">     *    +  如果path是一个目录但递归删除为false,则会则抛出异常。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">boolean</span> result = fileSystem.<span class="keyword">delete</span>(<span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/b.txt"</span>), <span class="keyword">true</span>);</span><br><span class="line">    System.out.<span class="keyword">println</span>(result);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-9-上传文件到HDFS"><a href="#2-9-上传文件到HDFS" class="headerlink" title="2.9 上传文件到HDFS"></a>2.9 上传文件到HDFS</h3><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">copyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 如果指定的是目录，则会把目录及其中的文件都复制到指定目录下</span></span><br><span class="line">    Path src = <span class="keyword">new</span> Path(<span class="string">"D:\\BigData-Notes\\notes\\installation"</span>);</span><br><span class="line">    Path dst = <span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/"</span>);</span><br><span class="line">    fileSystem.copyFromLocalFile(src, dst);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-10-上传大文件并显示上传进度"><a href="#2-10-上传大文件并显示上传进度" class="headerlink" title="2.10 上传大文件并显示上传进度"></a>2.10 上传大文件并显示上传进度</h3><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">copyFromLocalBigFile</span>(<span class="params"></span>) throws Exception</span> &#123;</span><br><span class="line"></span><br><span class="line">        File file = <span class="keyword">new</span> File(<span class="string">"D:\\kafka.tgz"</span>);</span><br><span class="line">        final <span class="keyword">float</span> fileSize = file.length();</span><br><span class="line">        InputStream <span class="keyword">in</span> = <span class="keyword">new</span> BufferedInputStream(<span class="keyword">new</span> FileInputStream(file));</span><br><span class="line"></span><br><span class="line">        FSDataOutputStream <span class="keyword">out</span> = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/kafka5.tgz"</span>),</span><br><span class="line">                <span class="keyword">new</span> Progressable() &#123;</span><br><span class="line">                  <span class="keyword">long</span> fileCount = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">                  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">progress</span>(<span class="params"></span>)</span> &#123;</span><br><span class="line">                     fileCount++;</span><br><span class="line">                     <span class="comment">// progress方法每上传大约64KB的数据后就会被调用一次</span></span><br><span class="line">                     System.<span class="keyword">out</span>.println(<span class="string">"上传进度："</span> + (fileCount * <span class="number">64</span> * <span class="number">1024</span> / fileSize) * <span class="number">100</span> + <span class="string">" %"</span>);</span><br><span class="line">                   &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        IOUtils.copyBytes(<span class="keyword">in</span>, <span class="keyword">out</span>, <span class="number">4096</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-11-从HDFS上下载文件"><a href="#2-11-从HDFS上下载文件" class="headerlink" title="2.11 从HDFS上下载文件"></a>2.11 从HDFS上下载文件</h3><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">copyToLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Path src = <span class="keyword">new</span> Path(<span class="string">"/hdfs-api/test/kafka.tgz"</span>);</span><br><span class="line">    Path dst = <span class="keyword">new</span> Path(<span class="string">"D:\\app\\"</span>);</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 第一个参数控制下载完成后是否删除源文件,默认是true,即删除;</span></span><br><span class="line"><span class="comment">     * 最后一个参数表示是否将RawLocalFileSystem用作本地文件系统;</span></span><br><span class="line"><span class="comment">     * RawLocalFileSystem默认为false,通常情况下可以不设置,</span></span><br><span class="line"><span class="comment">     * 但如果你在执行时候抛出NullPointerException异常,则代表你的文件系统与程序可能存在不兼容的情况(window下常见),</span></span><br><span class="line"><span class="comment">     * 此时可以将RawLocalFileSystem设置为true</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    fileSystem.copyToLocalFile(<span class="keyword">false</span>, src, dst, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-12-查看指定目录下所有文件的信息"><a href="#2-12-查看指定目录下所有文件的信息" class="headerlink" title="2.12 查看指定目录下所有文件的信息"></a>2.12 查看指定目录下所有文件的信息</h3><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">listFiles</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    FileStatus[] statuses = fileSystem.listStatus(<span class="keyword">new</span> Path(<span class="string">"/hdfs-api"</span>));</span><br><span class="line">    <span class="keyword">for</span> (FileStatus fileStatus : statuses) &#123;</span><br><span class="line">        <span class="comment">//fileStatus的toString方法被重写过，直接打印可以看到所有信息</span></span><br><span class="line">        System.out.println(fileStatus.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>FileStatus</code>中包含了文件的基本信息，比如文件路径，是否是文件夹，修改时间，访问时间，所有者，所属组，文件权限，是否是符号链接等，输出内容示例如下：</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">FileStatus&#123;</span><br><span class="line"><span class="attribute">path</span>=hdfs://192.168.0.106:8020/hdfs-api/test; </span><br><span class="line"><span class="attribute">isDirectory</span>=<span class="literal">true</span>; </span><br><span class="line"><span class="attribute">modification_time</span>=1556680796191; </span><br><span class="line"><span class="attribute">access_time</span>=0; </span><br><span class="line"><span class="attribute">owner</span>=root; </span><br><span class="line"><span class="attribute">group</span>=supergroup; </span><br><span class="line"><span class="attribute">permission</span>=rwxr-xr-x; </span><br><span class="line"><span class="attribute">isSymlink</span>=<span class="literal">false</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-13-递归查看指定目录下所有文件的信息"><a href="#2-13-递归查看指定目录下所有文件的信息" class="headerlink" title="2.13 递归查看指定目录下所有文件的信息"></a>2.13 递归查看指定目录下所有文件的信息</h3><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">listFilesRecursive</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; files = fileSystem.listFiles(<span class="keyword">new</span> Path(<span class="string">"/hbase"</span>), <span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">while</span> (files.hasNext()) &#123;</span><br><span class="line">        System.out.println(files.next());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>和上面输出类似，只是多了文本大小，副本系数，块大小信息。</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">LocatedFileStatus&#123;</span><br><span class="line"><span class="attribute">path</span>=hdfs://192.168.0.106:8020/hbase/hbase.version; </span><br><span class="line"><span class="attribute">isDirectory</span>=<span class="literal">false</span>; </span><br><span class="line"><span class="attribute">length</span>=7; </span><br><span class="line"><span class="attribute">replication</span>=1; </span><br><span class="line"><span class="attribute">blocksize</span>=134217728; </span><br><span class="line"><span class="attribute">modification_time</span>=1554129052916; </span><br><span class="line"><span class="attribute">access_time</span>=1554902661455; </span><br><span class="line"><span class="attribute">owner</span>=root; <span class="attribute">group</span>=supergroup;</span><br><span class="line"><span class="attribute">permission</span>=rw-r--r--; </span><br><span class="line"><span class="attribute">isSymlink</span>=<span class="literal">false</span>&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-14-查看文件的块信息"><a href="#2-14-查看文件的块信息" class="headerlink" title="2.14 查看文件的块信息"></a>2.14 查看文件的块信息</h3><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void getFileBlockLocations() throws Exception &#123;</span><br><span class="line"></span><br><span class="line">    FileStatus fileStatus = fileSystem.getFileStatus(new Path(<span class="string">"/hdfs-api/test/kafka.tgz"</span>))<span class="comment">;</span></span><br><span class="line">    <span class="keyword">BlockLocation[] </span><span class="keyword">blocks </span>= fileSystem.getFileBlockLocations(fileStatus, <span class="number">0</span>, fileStatus.getLen())<span class="comment">;</span></span><br><span class="line">    for (<span class="keyword">BlockLocation </span><span class="keyword">block </span>: <span class="keyword">blocks) </span>&#123;</span><br><span class="line">        System.out.println(<span class="keyword">block);</span></span><br><span class="line"><span class="keyword"> </span>   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>块输出信息有三个值，分别是文件的起始偏移量(offset)，文件大小(length)，块所在的主机名(hosts)。</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span>,<span class="number">57028557</span>,hadoop001</span><br></pre></td></tr></table></figure>
<p><strong>以上所有测试用例下载地址</strong>：<a href="https://github.com/myhhub/BigData-Notes/tree/master/code/Hadoop/hdfs-java-api" target="_blank" rel="noopener">HDFS Java API</a></p>

    </div>
    
    <div class="post-footer">
        <div>
            
                转载声明：商业转载请联系作者获得授权,非商业转载请注明出处 © <a href="/" target="_blank">Ljjyy.com</a>
            
        </div>
        <div>
            
        </div>
    </div>
</article>

<div class="article-nav prev-next-wrap clearfix">
    
        <a href="/archives/2019/06/100318.html" class="pre-post btn btn-default" title='大数据hadoop之 十二.SSH免密钥登录'>
            <i class="fa fa-angle-left fa-fw"></i><span class="hidden-lg">上一篇</span>
            <span class="hidden-xs">大数据hadoop之 十二.SSH免密钥登录</span>
        </a>
    
    
        <a href="/archives/2019/06/100316.html" class="next-post btn btn-default" title='大数据hadoop之 十.Hadoop的完全分布式搭建'>
            <span class="hidden-lg">下一篇</span>
            <span class="hidden-xs">大数据hadoop之 十.Hadoop的完全分布式搭建</span><i class="fa fa-angle-right fa-fw"></i>
        </a>
    
</div>


    <div id="comments">
        
	
    <div id="vcomments" class="valine"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="/assets/valine.min.js"></script>

    <script>
        new Valine({
            av: AV,
            el: '#vcomments',
            appId: '5MzTXYXkt03k101j0PmSDN34-gzGzoHsz',
            appKey: 'iwjYgwno6qj3wtDVVSbe8nYQ',
            placeholder: '说点什么吧',
            notify: false,
            verify: true,
            avatar: 'mm',
            meta: 'nick,mail'.split(','),
            pageSize: '10',
            path: window.location.pathname,
            lang: 'zh-CN'.toLowerCase()
        })
    </script>


    </div>





                </main>
                
                    <aside id="article-toc" role="navigation" class="col-md-4">
    <div class="widget">
        <h3 class="title">文章目录</h3>
        
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS常用Shell命令"><span class="toc-text">HDFS常用Shell命令</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS文件操作"><span class="toc-text">HDFS文件操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#基本文件fs命令"><span class="toc-text">基本文件fs命令</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-显示当前目录结构"><span class="toc-text">1. 显示当前目录结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-创建目录"><span class="toc-text">2. 创建目录</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-从本地加载文件到HDFS"><span class="toc-text">4. 从本地加载文件到HDFS</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-从HDFS导出文件到本地"><span class="toc-text">5. 从HDFS导出文件到本地</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-显示文件的最后一千字节"><span class="toc-text">7. 显示文件的最后一千字节</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-拷贝文件"><span class="toc-text">8. 拷贝文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-移动文件"><span class="toc-text">9. 移动文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-统计当前目录下各文件大小"><span class="toc-text">10. 统计当前目录下各文件大小</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11-合并下载多个文件"><span class="toc-text">11. 合并下载多个文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-统计文件系统的可用空间信息"><span class="toc-text">12. 统计文件系统的可用空间信息</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#13-更改文件复制因子"><span class="toc-text">13. 更改文件复制因子</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-权限控制"><span class="toc-text">14. 权限控制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#15-文件检测"><span class="toc-text">15. 文件检测</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cat命令"><span class="toc-text">cat命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#chgrp命令"><span class="toc-text">chgrp命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#chmod命令"><span class="toc-text">chmod命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#chown命令"><span class="toc-text">chown命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cp命令"><span class="toc-text">cp命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#du命令"><span class="toc-text">du命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#expunge命令"><span class="toc-text">expunge命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#get命令"><span class="toc-text">get命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lsr命令"><span class="toc-text">lsr命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mv命令"><span class="toc-text">mv命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#put命令"><span class="toc-text">put命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rmr命令"><span class="toc-text">rmr命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#job命令"><span class="toc-text">job命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#系统体检"><span class="toc-text">系统体检</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#HDFS-Java-API的使用"><span class="toc-text">HDFS Java API的使用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#一、-简介"><span class="toc-text">一、 简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#二、API的使用"><span class="toc-text">二、API的使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-FileSystem"><span class="toc-text">2.1 FileSystem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-创建目录"><span class="toc-text">2.2 创建目录</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-创建指定权限的目录"><span class="toc-text">2.3 创建指定权限的目录</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-创建文件，并写入内容"><span class="toc-text">2.4 创建文件，并写入内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-判断文件是否存在"><span class="toc-text">2.5 判断文件是否存在</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-查看文件内容"><span class="toc-text">2.6 查看文件内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-7-文件重命名"><span class="toc-text">2.7 文件重命名</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-8-删除目录或文件"><span class="toc-text">2.8 删除目录或文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-9-上传文件到HDFS"><span class="toc-text">2.9 上传文件到HDFS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-10-上传大文件并显示上传进度"><span class="toc-text">2.10 上传大文件并显示上传进度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-11-从HDFS上下载文件"><span class="toc-text">2.11 从HDFS上下载文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-12-查看指定目录下所有文件的信息"><span class="toc-text">2.12 查看指定目录下所有文件的信息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-13-递归查看指定目录下所有文件的信息"><span class="toc-text">2.13 递归查看指定目录下所有文件的信息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-14-查看文件的块信息"><span class="toc-text">2.14 查看文件的块信息</span></a></li></ol></li></ol></li></ol>
        
    </div>
</aside>

                
            </div>
        </div>
    </section>
    <footer class="main-footer">
    <div class="container">
        <div class="row">
        </div>
    </div>
</footer>

<a id="back-to-top" class="icon-btn hide">
	<i class="fa fa-chevron-up"></i>
</a>




    <div class="copyright">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="busuanzi">
    
</div>

            </div>
            <div class="col-sm-12">
                <span>Copyright &copy; 2019-2023&emsp;<a href="/" class="copyright-links" target="_blank" rel="nofollow">Ljjyy.com</a>
                </span> |
                <span>
                    <a href="/about/" class="copyright-links" target="_blank" rel="nofollow">关于我们</a>
                </span> |                
                <span>
                    <a href="/sitemap.xml" class="copyright-links" target="_blank" rel="nofollow">网站地图</a>
                </span> |
                <span>
                    <a href="/archives/" class="copyright-links" target="_blank" rel="nofollow">时间轴</a>
                </span>              
            </div>
        </div>
    </div>
</div>







<script src="/js/app.js?rev=@@hash"></script>

</body>
</html>