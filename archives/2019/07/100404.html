<!DOCTYPE HTML>
<html lang="zh-CN">
<head>
    <!--Setting-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta http-equiv="Cache-Control" content="no-siteapp">
    <meta http-equiv="Cache-Control" content="no-transform">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="apple-mobile-web-app-capable" content="来唧唧歪歪(Ljjyy.com) - 多读书多实践，勤思考善领悟">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    <meta name="browsermode" content="application">
    <meta name="screen-orientation" content="portrait">
    <meta name="theme-version" content="1.2.3">
    <meta name="root" content="/">
    <link rel="dns-prefetch" href="https://www.ljjyy.com">
    <!--SEO-->

    <meta name="keywords" content="hadoop,spark" />


    <meta name="description" content="一、作业提交1.1  spark-submitSpark所有模式均使用spark-submit命令提交作业，其格式如下：
12345678./bin/spark-submit \  --clas..." />



<meta name="robots" content="all" />
<meta name="google" content="all" />
<meta name="googlebot" content="all" />
<meta name="verify" content="all" />

    <!--Title-->


<title>Spark部署模式与作业提交 | 来唧唧歪歪(Ljjyy.com) - 多读书多实践，勤思考善领悟</title>


    <link rel="alternate" href="/atom.xml" title="来唧唧歪歪(Ljjyy.com) - 多读书多实践，勤思考善领悟" type="application/atom+xml">


    <link rel="icon" href="/favicon.ico">

    




<link rel="stylesheet" href="/css/bootstrap.min.css?rev=3.3.7.css">
<link rel="stylesheet" href="/css/font-awesome.min.css?rev=4.5.0.css">
<link rel="stylesheet" href="/css/style.css?rev=@@hash.css">





    
	<div class="hide">
        <script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script>
        <script>LA.init({id: "JgbNOaw1xxsmUUsQ",ck: "JgbNOaw1xxsmUUsQ"})</script>
	</div>






    
    <meta name="baidu-site-verification" content="dTHILoORpx" />


    <script>
        (function(){
            var bp = document.createElement('script');
            var curProtocol = window.location.protocol.split(':')[0];
            if (curProtocol === 'https') {
                bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
            }
            else {
                bp.src = 'http://push.zhanzhang.baidu.com/push.js';
            }
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(bp, s);
        })();
    </script>

<meta name="generator" content="Hexo 7.3.0"></head>


<!--[if lte IE 8]>
<style>
    html{ font-size: 1em }
</style>
<![endif]-->
<!--[if lte IE 9]>
<div style="ie">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div>
<![endif]-->

<body>
    <header class="main-header"  >
    <div class="main-header-box">
        <!--a class="header-avatar" href="/" title='Ljjyy.com'>
            <img src="/img/avatar.jpg" alt="logo头像" class="img-responsive center-block">
        </a-->
        <div class="branding">
            
                <h2> 多读书多实践，勤思考善领悟 </h2>
            
    	  </div>
    </div>
</header>
    <nav class="main-navigation">
    <div class="container">

        <div class="row">
            <div class="col-sm-12">
                <div class="navbar-header"><span class="nav-toggle-button collapsed pull-right" data-toggle="collapse" data-target="#main-menu" id="mnav">
                    <span class="sr-only"></span>
                        <i class="fa fa-bars"></i>
                    </span>
                    <a class="web-logo"  href="/" title='Ljjyy.com'></a>
                    <!--a class="navbar-brand" href="https://www.ljjyy.com">来唧唧歪歪(Ljjyy.com) - 多读书多实践，勤思考善领悟</a-->
                </div>
                <div class="collapse navbar-collapse" id="main-menu" style="">
                    <ul class="menu">
                        
                            <li role="presentation" class="text-center">
                                <a href="/"><i class="fa "></i>首页</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/cloud/"><i class="fa "></i>云计算</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/front/"><i class="fa "></i>前端</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/back/"><i class="fa "></i>后端</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/devops/"><i class="fa "></i>运维</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/crack/"><i class="fa "></i>破解</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/penetration/"><i class="fa "></i>渗透</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/tool/"><i class="fa "></i>工具</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/other/"><i class="fa "></i>其他</a>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </div>
    </div>
</nav>
    <section class="content-wrap">
        <div class="container">
            <div class="row">
                <main class="col-md-8 main-content m-post">
                    <p id="process"></p>
<article class="post">
    <div class="post-head">
        <h1 id="Spark部署模式与作业提交">
            
	            Spark部署模式与作业提交
            
        </h1>
        <div class="post-meta">
    
        <span class="categories-meta fa-wrap">
            <i class="fa fa-folder-open-o"></i>
            <a class="category-link" href="/categories/cloud/">云计算</a>
        </span>
    

    
        <span class="fa-wrap">
            <i class="fa fa-tags"></i>
            <span class="tags-meta">
                
                    <a class="tag-none-link" href="/tags/hadoop/" rel="tag">hadoop</a> <a class="tag-none-link" href="/tags/spark/" rel="tag">spark</a>
                
            </span>
        </span>
    

    
        
        <span class="fa-wrap">
            <i class="fa fa-clock-o"></i>
            <span class="date-meta">2019/07/10</span>
        </span>
        
    
</div>
            
            
            <p class="fa fa-exclamation-triangle warning">
                本文于<strong>2078</strong>天之前发表，文中内容可能已经过时。
            </p>
        
    </div>
    
    <div class="post-body post-content">
        <h2 id="一、作业提交"><a href="#一、作业提交" class="headerlink" title="一、作业提交"></a>一、作业提交</h2><h3 id="1-1-spark-submit"><a href="#1-1-spark-submit" class="headerlink" title="1.1  spark-submit"></a>1.1  spark-submit</h3><p>Spark所有模式均使用<code>spark-submit</code>命令提交作业，其格式如下：</p>
<figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit <span class="string">\</span></span><br><span class="line">  --<span class="keyword">class</span> &lt;main-<span class="keyword">class</span>&gt; <span class="string">\</span>        <span class="comment"># 应用程序主入口类</span></span><br><span class="line">  --master &lt;master-url&gt; <span class="string">\</span>       <span class="comment"># 集群的Master Url</span></span><br><span class="line">  --deploy-mode &lt;deploy-mode&gt; <span class="string">\</span> <span class="comment"># 部署模式</span></span><br><span class="line">  --conf &lt;key&gt;=&lt;value&gt; <span class="string">\</span>        <span class="comment"># 可选配置       </span></span><br><span class="line">  ... <span class="comment"># other options    </span></span><br><span class="line">  &lt;application-jar&gt; <span class="string">\</span>           <span class="comment"># Jar包路径 </span></span><br><span class="line">  [application-arguments]       <span class="comment">#传递给主入口类的参数  </span></span><br></pre></td></tr></table></figure>

<p>需要注意的是：在集群环境下，<code>application-jar</code>必须能被集群中所有节点都能访问，可以是HDFS上的路径；也可以是本地文件系统路径，如果是本地文件系统路径，则要求集群中每一个机器节点上的相同路径都存在该Jar包。</p>
<h3 id="1-2-deploy-mode"><a href="#1-2-deploy-mode" class="headerlink" title="1.2 deploy-mode"></a>1.2 deploy-mode</h3><p>deploy-mode有<code>cluster</code>和<code>client</code>两个可选参数，默认为<code>client</code>。这里以Spark On Yarn模式对两者的区别进行说明 ：</p>
<ul>
<li>在cluster模式下，Spark Drvier在应用程序的Master进程内运行，该进程由群集上的YARN管理，提交作业的客户端可以在启动应用程序后关闭；</li>
<li>在client模式下，Spark Drvier在提交作业的客户端进程中运行，Master进程仅用于从YARN请求资源。</li>
</ul>
<h3 id="1-3-master-url"><a href="#1-3-master-url" class="headerlink" title="1.3 master-url"></a>1.3 master-url</h3><p>master-url的所有可选参数如下表所示：</p>
<table>
<thead>
<tr>
<th>Master URL</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td><code>local</code></td>
<td>使用一个线程本地运行Spark</td>
</tr>
<tr>
<td><code>local[K]</code></td>
<td>使用 K 个 worker 线程本地运行 Spark</td>
</tr>
<tr>
<td><code>local[K,F]</code></td>
<td>使用 K 个 worker 线程本地运行 , 第二个参数为Task的失败重试次数</td>
</tr>
<tr>
<td><code>local[*]</code></td>
<td>使用与CPU核心数一样的线程数在本地运行Spark</td>
</tr>
<tr>
<td><code>local[*,F]</code></td>
<td>使用与CPU核心数一样的线程数在本地运行Spark<br/>第二个参数为Task的失败重试次数</td>
</tr>
<tr>
<td><code>spark://HOST:PORT</code></td>
<td>连接至指定的 standalone 集群的 master 节点。端口号默认是 7077。</td>
</tr>
<tr>
<td><code>spark://HOST1:PORT1,HOST2:PORT2</code></td>
<td>如果standalone集群采用Zookeeper实现高可用，则必须包含由zookeeper设置的所有master主机地址。</td>
</tr>
<tr>
<td><code>mesos://HOST:PORT</code></td>
<td>连接至给定的Mesos集群。端口默认是 5050。对于使用了 ZooKeeper 的 Mesos cluster 来说，使用 <code>mesos://zk://...</code>来指定地址，使用 <code>--deploy-mode cluster</code>模式来提交。</td>
</tr>
<tr>
<td><code>yarn</code></td>
<td>连接至一个 YARN 集群，集群由配置的 <code>HADOOP_CONF_DIR</code> 或者 <code>YARN_CONF_DIR</code> 来决定。使用<code>--deploy-mode</code>参数来配置<code>client</code> 或<code>cluster</code> 模式。</td>
</tr>
</tbody></table>
<p>下面主要介绍三种常用部署模式及对应的作业提交方式。</p>
<h2 id="二、Local模式"><a href="#二、Local模式" class="headerlink" title="二、Local模式"></a>二、Local模式</h2><p>Local模式下提交作业最为简单，不需要进行任何配置，提交命令如下：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 本地模式提交应用</span></span><br><span class="line"><span class="attribute">spark</span>-submit <span class="punctuation">\</span></span><br><span class="line"><span class="punctuation"></span>--class org.apache.spark.examples.SparkPi <span class="punctuation">\</span></span><br><span class="line"><span class="punctuation"></span>--master local[<span class="number">2</span>] <span class="punctuation">\</span></span><br><span class="line"><span class="punctuation"></span>/usr/app/spark-<span class="number">2</span>.<span class="number">4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/examples/jars/spark-examples_2.<span class="number">11</span>-<span class="number">2</span>.<span class="number">4</span>.<span class="number">0</span>.jar <span class="punctuation">\</span></span><br><span class="line"><span class="punctuation"></span><span class="number">100</span>   # 传给SparkPi的参数</span><br></pre></td></tr></table></figure>

<p><code>spark-examples_2.11-2.4.0.jar</code>是Spark提供的测试用例包，<code>SparkPi</code>用于计算Pi值，执行结果如下：</p>
<div align="center"> <img src="/img/bigdata/spark-pi.png"/> </div>



<h2 id="三、Standalone模式"><a href="#三、Standalone模式" class="headerlink" title="三、Standalone模式"></a>三、Standalone模式</h2><p>Standalone是Spark提供的一种内置的集群模式，采用内置的资源管理器进行管理。下面按照如图所示演示1个Mater和2个Worker节点的集群配置，这里使用两台主机进行演示：</p>
<ul>
<li>hadoop001： 由于只有两台主机，所以hadoop001既是Master节点，也是Worker节点;</li>
<li>hadoop002 ： Worker节点。</li>
</ul>
<div align="center"> <img src="/img/bigdata/spark-jqms.png"/> </div>

<h3 id="3-1-环境配置"><a href="#3-1-环境配置" class="headerlink" title="3.1 环境配置"></a>3.1 环境配置</h3><p>首先需要保证Spark已经解压在两台主机的相同路径上。然后进入hadoop001的<code>$&#123;SPARK_HOME&#125;/conf/</code>目录下，拷贝配置样本并进行相关配置：</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># <span class="keyword">cp</span> spark-env.<span class="keyword">sh</span>.template spark-env.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure>

<p>在<code>spark-env.sh</code>中配置JDK的目录，完成后将该配置使用scp命令分发到hadoop002上：</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># JDK安装位置</span></span><br><span class="line"><span class="attribute">JAVA_HOME</span>=/usr/java/jdk1.<span class="number">8</span>.<span class="number">0</span>_201</span><br></pre></td></tr></table></figure>

<h3 id="3-2-集群配置"><a href="#3-2-集群配置" class="headerlink" title="3.2 集群配置"></a>3.2 集群配置</h3><p>在<code>$&#123;SPARK_HOME&#125;/conf/</code>目录下，拷贝集群配置样本并进行相关配置：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">cp</span> slaves.template slaves</span></span><br></pre></td></tr></table></figure>

<p>指定所有Worker节点的主机名：</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># A Spark Worker will be started on each of the machines listed below.</span></span><br><span class="line"><span class="attribute">hadoop001</span></span><br><span class="line">hadoop002</span><br></pre></td></tr></table></figure>

<p>这里需要注意以下三点：</p>
<ul>
<li>主机名与IP地址的映射必须在<code>/etc/hosts</code>文件中已经配置，否则就直接使用IP地址；</li>
<li>每个主机名必须独占一行；</li>
<li>Spark的Master主机是通过SSH访问所有的Worker节点，所以需要预先配置免密登录。</li>
</ul>
<h3 id="3-3-启动"><a href="#3-3-启动" class="headerlink" title="3.3 启动"></a>3.3 启动</h3><p>使用<code>start-all.sh</code>代表启动Master和所有Worker服务。</p>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/<span class="literal">start</span>-<span class="literal">master</span>.sh </span><br></pre></td></tr></table></figure>

<p>访问8080端口，查看Spark的Web-UI界面,，此时应该显示有两个有效的工作节点：</p>
<div align="center"> <img src="/img/bigdata/spark-Standalone-web-ui.png"/> </div>

<h3 id="3-4-提交作业"><a href="#3-4-提交作业" class="headerlink" title="3.4 提交作业"></a>3.4 提交作业</h3><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以client模式提交到standalone集群 </span></span><br><span class="line">spark-submit \</span><br><span class="line"><span class="params">--class</span> org.apache.spark.examples.SparkPi \</span><br><span class="line"><span class="params">--master</span> spark:<span class="string">//hadoop001</span><span class="function">:7077</span> \</span><br><span class="line"><span class="params">--executor-memory</span> 2G \</span><br><span class="line"><span class="params">--total-executor-cores</span> 10 \</span><br><span class="line"><span class="string">/usr/app/spark-2.4.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.4.0.jar</span> \</span><br><span class="line">100</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以cluster模式提交到standalone集群 </span></span><br><span class="line">spark-submit \</span><br><span class="line"><span class="params">--class</span> org.apache.spark.examples.SparkPi \</span><br><span class="line"><span class="params">--master</span> spark:<span class="string">//207.184.161.138</span><span class="function">:7077</span> \</span><br><span class="line"><span class="params">--deploy-mode</span> cluster \</span><br><span class="line"><span class="params">--supervise</span> \  <span class="comment"># 配置此参数代表开启监督，如果主应用程序异常退出，则自动重启Driver</span></span><br><span class="line"><span class="params">--executor-memory</span> 2G \</span><br><span class="line"><span class="params">--total-executor-cores</span> 10 \</span><br><span class="line"><span class="string">/usr/app/spark-2.4.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.4.0.jar</span> \</span><br><span class="line">100</span><br></pre></td></tr></table></figure>

<h3 id="3-5-可选配置"><a href="#3-5-可选配置" class="headerlink" title="3.5 可选配置"></a>3.5 可选配置</h3><p>在虚拟机上提交作业时经常出现一个的问题是作业无法申请�足够的资源：&#96;&#96;&#96;es<br>Initial job has not accepted any resources;<br>check your cluster UI to ensure that workers are registered and have sufficient resources</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="variable">&lt;div align=&quot;center&quot;&gt;</span> <span class="variable">&lt;img src=&quot;/img/bigdata/spark-rcbz2.png&quot;/&gt;</span> <span class="variable">&lt;/div&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="variable">&lt;br/&gt;</span></span><br><span class="line"></span><br><span class="line">这时候可以查看Web UI，我这里是内存空间不足：提交命令中要求作业的`executor-memory`是2G，但是实际的工作节点的`Memory`只有1G，这时候你可以修改`--executor-memory`，也可以修改 Woker 的`Memory`，其默认值为主机所有可用内存值减去1G。</span><br><span class="line"></span><br><span class="line"><span class="variable">&lt;div align=&quot;center&quot;&gt;</span> <span class="variable">&lt;img src=&quot;/img/bigdata/spark-rcbz.png&quot;/&gt;</span> <span class="variable">&lt;/div&gt;</span>   </span><br><span class="line"></span><br><span class="line"><span class="variable">&lt;br/&gt;</span></span><br><span class="line"></span><br><span class="line">关于Master和Woker节点的所有可选配置如下，可以在`spark-env.sh`中进行对应的配置：    </span><br><span class="line"></span><br><span class="line">|<span class="string"> Environment Variable（环境变量） </span>|<span class="string"> Meaning（含义）                                              </span>|</span><br><span class="line">|<span class="string"> -------------------------------- </span>|<span class="string"> ------------------------------------------------------------ </span>|</span><br><span class="line">|<span class="string"> `SPARK_MASTER_HOST`              </span>|<span class="string"> master 节点地址                                              </span>|</span><br><span class="line">|<span class="string"> `SPARK_MASTER_PORT`              </span>|<span class="string"> master 节点地址端口（默认：7077）                            </span>|</span><br><span class="line">|<span class="string"> `SPARK_MASTER_WEBUI_PORT`        </span>|<span class="string"> master 的 web UI 的端口（默认：8080）                        </span>|</span><br><span class="line">|<span class="string"> `SPARK_MASTER_OPTS`              </span>|<span class="string"> 仅用于 master 的配置属性，格式是 &quot;-Dx=y&quot;（默认：none）,所有属性可以参考官方文档：[spark-standalone-mode](https://spark.apache.org/docs/latest/spark-standalone.html#spark-standalone-mode) </span>|</span><br><span class="line">|<span class="string"> `SPARK_LOCAL_DIRS`               </span>|<span class="string"> spark 的临时存储的目录，用于暂存map的输出和持久化存储RDDs。多个目录用逗号分隔 </span>|</span><br><span class="line">|<span class="string"> `SPARK_WORKER_CORES`             </span>|<span class="string"> spark worker节点可以使用CPU Cores的数量。（默认：全部可用）  </span>|</span><br><span class="line">|<span class="string"> `SPARK_WORKER_MEMORY`            </span>|<span class="string"> spark worker节点可以使用的内存数量（默认：全部的内存减去1GB）； </span>|</span><br><span class="line">|<span class="string"> `SPARK_WORKER_PORT`              </span>|<span class="string"> spark worker节点的端口（默认： random（随机））              </span>|</span><br><span class="line">|<span class="string"> `SPARK_WORKER_WEBUI_PORT`        </span>|<span class="string"> worker 的 web UI 的 Port（端口）（默认：8081）               </span>|</span><br><span class="line">|<span class="string"> `SPARK_WORKER_DIR`               </span>|<span class="string"> worker运行应用程序的目录，这个目录中包含日志和暂存空间（default：SPARK_HOME/work） </span>|</span><br><span class="line">|<span class="string"> `SPARK_WORKER_OPTS`              </span>|<span class="string"> 仅用于 worker 的配置属性，格式是 &quot;-Dx=y&quot;（默认：none）。所有属性可以参考官方文档：[spark-standalone-mode](https://spark.apache.org/docs/latest/spark-standalone.html#spark-standalone-mode) </span>|</span><br><span class="line">|<span class="string"> `SPARK_DAEMON_MEMORY`            </span>|<span class="string"> 分配给 spark master 和 worker 守护进程的内存。（默认： 1G）  </span>|</span><br><span class="line">|<span class="string"> `SPARK_DAEMON_JAVA_OPTS`         </span>|<span class="string"> spark master 和 worker 守护进程的 JVM 选项，格式是 &quot;-Dx=y&quot;（默认：none） </span>|</span><br><span class="line">|<span class="string"> `SPARK_PUBLIC_DNS`               </span>|<span class="string"> spark master 和 worker 的公开 DNS 名称。（默认：none）       </span>|</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 三、Spark on Yarn模式</span></span><br><span class="line"></span><br><span class="line">Spark支持将作业提交到Yarn上运行，此时不需要启动Master节点，也不需要启动Worker节点。</span><br><span class="line"></span><br><span class="line"><span class="comment">### 3.1 配置</span></span><br><span class="line"></span><br><span class="line">在`spark-env.sh`中配置hadoop的配置目录的位置，可以使用`YARN_CONF_DIR`或`HADOOP_CONF_DIR`进行指定：</span><br></pre></td></tr></table></figure>
<p>YARN_CONF_DIR&#x3D;&#x2F;usr&#x2F;app&#x2F;hadoop-2.6.0-cdh5.15.2&#x2F;etc&#x2F;hadoop</p>
<h1 id="JDK安装位置"><a href="#JDK安装位置" class="headerlink" title="JDK安装位置"></a>JDK安装位置</h1><p>JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_201</p>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### <span class="number">3.2</span> 启动</span><br><span class="line"></span><br><span class="line">必须要保证Hadoop已经启动，这里包括YARN和HDFS都需要启动，因为在计算过程中Spark会使用HDFS存储临时文件，如果HDFS没有启动，则会抛出异常。</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="start-yarn-sh"><a href="#start-yarn-sh" class="headerlink" title="start-yarn.sh"></a>start-yarn.sh</h1><h1 id="start-dfs-sh"><a href="#start-dfs-sh" class="headerlink" title="start-dfs.sh"></a>start-dfs.sh</h1><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### <span class="number">3.3</span> 提交应用</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="以client模式提交到yarn集群"><a href="#以client模式提交到yarn集群" class="headerlink" title="以client模式提交到yarn集群"></a>以client模式提交到yarn集群</h1><p>spark-submit <br>–class org.apache.spark.examples.SparkPi <br>–master yarn <br>–deploy-mode client <br>–executor-memory 2G <br>–num-executors 10 <br>&#x2F;usr&#x2F;app&#x2F;spark-2.4.0-bin-hadoop2.6&#x2F;examples&#x2F;jars&#x2F;spark-examples_2.11-2.4.0.jar <br>100</p>
<h1 id="以cluster模式提交到yarn集群"><a href="#以cluster模式提交到yarn集群" class="headerlink" title="以cluster模式提交到yarn集群"></a>以cluster模式提交到yarn集群</h1><p>spark-submit <br>–class org.apache.spark.examples.SparkPi <br>–master yarn <br>–deploy-mode cluster <br>–executor-memory 2G <br>–num-executors 10 <br>&#x2F;usr&#x2F;app&#x2F;spark-2.4.0-bin-hadoop2.6&#x2F;examples&#x2F;jars&#x2F;spark-examples_2.11-2.4.0.jar <br>100</p>
<pre><code>



</code></pre>

    </div>
    
    <div class="post-footer">
        <div>
            
                转载声明：商业转载请联系作者获得授权,非商业转载请注明出处 © <a href="/" target="_blank">Ljjyy.com</a>
            
        </div>
        <div>
            
        </div>
    </div>
</article>

<div class="article-nav prev-next-wrap clearfix">
    
        <a href="/archives/2019/07/100405.html" class="pre-post btn btn-default" title='Spark累加器与广播变量'>
            <i class="fa fa-angle-left fa-fw"></i><span class="hidden-lg">上一篇</span>
            <span class="hidden-xs">Spark累加器与广播变量</span>
        </a>
    
    
        <a href="/archives/2019/07/100403.html" class="next-post btn btn-default" title='Spark Transformation 和 Action 常用算子'>
            <span class="hidden-lg">下一篇</span>
            <span class="hidden-xs">Spark Transformation 和 Action 常用算子</span><i class="fa fa-angle-right fa-fw"></i>
        </a>
    
</div>


    <div id="comments">
        
	
    <div id="vcomments" class="valine"></div>
    
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="/assets/valine.min.js"></script>


    <script>
        new Valine({
            av: AV,
            el: '#vcomments',
            appId: '5MzTXYXkt03k101j0PmSDN34-gzGzoHsz',
            appKey: 'iwjYgwno6qj3wtDVVSbe8nYQ',
            placeholder: '说点什么吧',
            notify: false,
            verify: true,
            avatar: 'mm',
            meta: 'nick,mail'.split(','),
            pageSize: '10',
            path: window.location.pathname,
            lang: 'zh-CN'.toLowerCase()
        })
    </script>


    </div>





                </main>
                
                    <aside id="article-toc" role="navigation" class="col-md-4">
    <div class="widget">
        <h3 class="title">文章目录</h3>
        
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4"><span class="toc-text">一、作业提交</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-spark-submit"><span class="toc-text">1.1  spark-submit</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-deploy-mode"><span class="toc-text">1.2 deploy-mode</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-master-url"><span class="toc-text">1.3 master-url</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Local%E6%A8%A1%E5%BC%8F"><span class="toc-text">二、Local模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81Standalone%E6%A8%A1%E5%BC%8F"><span class="toc-text">三、Standalone模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="toc-text">3.1 环境配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE"><span class="toc-text">3.2 集群配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%90%AF%E5%8A%A8"><span class="toc-text">3.3 启动</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E6%8F%90%E4%BA%A4%E4%BD%9C%E4%B8%9A"><span class="toc-text">3.4 提交作业</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-%E5%8F%AF%E9%80%89%E9%85%8D%E7%BD%AE"><span class="toc-text">3.5 可选配置</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#JDK%E5%AE%89%E8%A3%85%E4%BD%8D%E7%BD%AE"><span class="toc-text">JDK安装位置</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#start-yarn-sh"><span class="toc-text">start-yarn.sh</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#start-dfs-sh"><span class="toc-text">start-dfs.sh</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A5client%E6%A8%A1%E5%BC%8F%E6%8F%90%E4%BA%A4%E5%88%B0yarn%E9%9B%86%E7%BE%A4"><span class="toc-text">以client模式提交到yarn集群</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A5cluster%E6%A8%A1%E5%BC%8F%E6%8F%90%E4%BA%A4%E5%88%B0yarn%E9%9B%86%E7%BE%A4"><span class="toc-text">以cluster模式提交到yarn集群</span></a>
        
    </div>
</aside>

                
            </div>
        </div>
    </section>
    <footer class="main-footer">
    <div class="container">
        <div class="row">
        </div>
    </div>
</footer>

<a id="back-to-top" class="icon-btn hide">
	<i class="fa fa-chevron-up"></i>
</a>




    <div class="copyright">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="busuanzi">
    
</div>

            </div>
            <div class="col-sm-12">
                <span>Copyright &copy; 2019-2025&emsp;<a href="/" class="copyright-links" target="_blank" rel="nofollow">Ljjyy.com</a>
                </span> |
                <span>
                    <a href="/about/" class="copyright-links" target="_blank" rel="nofollow">关于我们</a>
                </span> |                
                <span>
                    <a href="/sitemap.xml" class="copyright-links" target="_blank" rel="nofollow">网站地图</a>
                </span> |
                <span>
                    <a href="/archives/" class="copyright-links" target="_blank" rel="nofollow">时间轴</a>
                </span>              
            </div>
        </div>
    </div>
</div>








<script src="/js/app.js?rev=@@hash.js"></script>


</body>
</html>