<!DOCTYPE HTML>
<html lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
    <!--Setting-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta http-equiv="Cache-Control" content="no-siteapp">
    <meta http-equiv="Cache-Control" content="no-transform">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="apple-mobile-web-app-capable" content="来唧唧歪歪(Ljjyy.com) - 多读书多实践，勤思考善领悟">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    <meta name="browsermode" content="application">
    <meta name="screen-orientation" content="portrait">
    <meta name="theme-version" content="1.2.3">
    <meta name="root" content="/">
    <link rel="dns-prefetch" href="https://www.ljjyy.com">
    <!--SEO-->

    <meta name="keywords" content="知识图谱,scrapy">


    <meta name="description" content="一. 简介本文章针对半结构化数据的获取，介绍基于scrapy构建的百度百科爬虫和互动百科爬虫。同时为了练手还根据教程制作了基于BeautifulSoup和urllib2的百度百科爬虫、微信公众号...">



<meta name="robots" content="all">
<meta name="google" content="all">
<meta name="googlebot" content="all">
<meta name="verify" content="all">

    <!--Title-->


<title>从零开始学习知识图谱 之 一.电影知识图谱构建 1.半结构化数据的获取 | 来唧唧歪歪(Ljjyy.com) - 多读书多实践，勤思考善领悟</title>


    <link rel="alternate" href="/atom.xml" title="来唧唧歪歪(Ljjyy.com) - 多读书多实践，勤思考善领悟" type="application/atom+xml">


    <link rel="icon" href="/favicon.ico">

    



<link rel="stylesheet" href="/css/bootstrap.min.css?rev=3.3.7">
<link rel="stylesheet" href="/css/font-awesome.min.css?rev=4.5.0">
<link rel="stylesheet" href="/css/style.css?rev=@@hash">




    
	<div class="hide">
        <script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script>
        <script>LA.init({id: "JgbNOaw1xxsmUUsQ",ck: "JgbNOaw1xxsmUUsQ"})</script>
	</div>






    
    <meta name="baidu-site-verification" content="dTHILoORpx">


    <script>
        (function(){
            var bp = document.createElement('script');
            var curProtocol = window.location.protocol.split(':')[0];
            if (curProtocol === 'https') {
                bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
            }
            else {
                bp.src = 'http://push.zhanzhang.baidu.com/push.js';
            }
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(bp, s);
        })();
    </script>

</head>

</html>
<!--[if lte IE 8]>
<style>
    html{ font-size: 1em }
</style>
<![endif]-->
<!--[if lte IE 9]>
<div style="ie">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div>
<![endif]-->

<body>
    <header class="main-header"  >
    <div class="main-header-box">
        <!--a class="header-avatar" href="/" title='Ljjyy.com'>
            <img src="/img/avatar.jpg" alt="logo头像" class="img-responsive center-block">
        </a-->
        <div class="branding">
            
                <h2> 多读书多实践，勤思考善领悟 </h2>
            
    	  </div>
    </div>
</header>
    <nav class="main-navigation">
    <div class="container">

        <div class="row">
            <div class="col-sm-12">
                <div class="navbar-header"><span class="nav-toggle-button collapsed pull-right" data-toggle="collapse" data-target="#main-menu" id="mnav">
                    <span class="sr-only"></span>
                        <i class="fa fa-bars"></i>
                    </span>
                    <a class="web-logo"  href="/" title='Ljjyy.com'></a>
                    <!--a class="navbar-brand" href="https://www.ljjyy.com">来唧唧歪歪(Ljjyy.com) - 多读书多实践，勤思考善领悟</a-->
                </div>
                <div class="collapse navbar-collapse" id="main-menu" style="">
                    <ul class="menu">
                        
                            <li role="presentation" class="text-center">
                                <a href="/"><i class="fa "></i>首页</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/cloud/"><i class="fa "></i>云计算</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/front/"><i class="fa "></i>前端</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/back/"><i class="fa "></i>后端</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/devops/"><i class="fa "></i>运维</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/crack/"><i class="fa "></i>破解</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/penetration/"><i class="fa "></i>渗透</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/tool/"><i class="fa "></i>工具</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/other/"><i class="fa "></i>其他</a>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </div>
    </div>
</nav>
    <section class="content-wrap">
        <div class="container">
            <div class="row">
                <main class="col-md-8 main-content m-post">
                    <p id="process"></p>
<article class="post">
    <div class="post-head">
        <h1 id="从零开始学习知识图谱 之 一.电影知识图谱构建 1.半结构化数据的获取">
            
	            从零开始学习知识图谱 之 一.电影知识图谱构建 1.半结构化数据的获取
            
        </h1>
        <div class="post-meta">
    
        <span class="categories-meta fa-wrap">
            <i class="fa fa-folder-open-o"></i>
            <a class="category-link" href="/categories/cloud/">云计算</a>
        </span>
    

    
        <span class="fa-wrap">
            <i class="fa fa-tags"></i>
            <span class="tags-meta">
                
                    <a class="tag-link" href="/tags/scrapy/">scrapy</a> <a class="tag-link" href="/tags/knowledgegraph/">知识图谱</a>
                
            </span>
        </span>
    

    
        
        <span class="fa-wrap">
            <i class="fa fa-clock-o"></i>
            <span class="date-meta">2019/10/17</span>
        </span>
        
    
</div>
            
            
            <p class="fa fa-exclamation-triangle warning">
                本文于<strong>1733</strong>天之前发表，文中内容可能已经过时。
            </p>
        
    </div>
    
    <div class="post-body post-content">
        <h1 id="一-简介"><a href="#一-简介" class="headerlink" title="一. 简介"></a>一. 简介</h1><p>本文章针对半结构化数据的获取，介绍基于scrapy构建的百度百科爬虫和互动百科爬虫。同时为了练手还根据教程制作了基于BeautifulSoup和urllib2的百度百科爬虫、微信公众号爬虫和虎嗅网爬虫。</p>
<p>目前百度百科爬虫，爬取电影类数据，包含电影22219部，演员13967人。互动百科爬虫， 爬取电影类数据，包含电影13866部，演员5931 人。</p>
<p>本教程的项目代码放在github上，下载地址为<a href="https://github.com/myhhub/zero_knowledge_graph" target="_blank" rel="noopener">《从零开始学习知识图谱》项目源代码</a> 。</p>
<h1 id="二-环境准备"><a href="#二-环境准备" class="headerlink" title="二. 环境准备"></a>二. 环境准备</h1><h2 id="1-操作系统"><a href="#1-操作系统" class="headerlink" title="1. 操作系统"></a>1. 操作系统</h2><p>支持操作系统：windows、macOS、Linux。为了方便大家搭建开发环境，笔者尽可能在windows下构建，系列篇未特意说明时操作系统都是<strong>windows</strong>。Linux安装可以参考<a href="100590.html">VirtualBox虚拟机安装Ubuntu</a>或<a href="100589.html">VirtualBox虚拟机安装CentOS8</a>进行安装。</p>
<h2 id="2-jdk"><a href="#2-jdk" class="headerlink" title="2. jdk"></a>2. jdk</h2><p>安装参见<a href="100604.html">windows系统安装JDK</a></p>
<h2 id="3-mysql"><a href="#3-mysql" class="headerlink" title="3. mysql"></a>3. mysql</h2><p>MySQL是一个关系型数据库管理系统，由瑞典MySQL AB 公司开发，目前属于 Oracle 旗下产品。MySQL 是最流行的关系型数据库管理系统之一，在 WEB 应用方面，MySQL是最好的 RDBMS (Relational Database Management System，关系数据库管理系统) 应用软件之一。</p>
<p>MySQL官网<a href="https://cdn.mysql.com//Downloads/MySQLInstaller/mysql-installer-community-8.0.17.0.msi" target="_blank" rel="noopener">安装包</a>，该安装包有安装指引，按步骤完成即可。</p>
<p>如果想安装MySQL绿色版（不建议），需要手工配置，请参考文章<a href="https://blog.csdn.net/qq_37350706/article/details/81707862" target="_blank" rel="noopener">MySQL 8.0.16安装教程(windows 64位)</a>。</p>
<h2 id="4-Python3"><a href="#4-Python3" class="headerlink" title="4. Python3"></a>4. Python3</h2><p>Python是一种跨平台的计算机程序设计语言。是一种面向对象的动态类型语言，最初被设计用于编写自动化脚本(shell)，随着版本的不断更新和语言新功能的添加，越来越多被用于独立的、大型项目的开发。</p>
<p>对于初学者和完成普通任务，Python语言是非常简单易用的。连Google都在大规模使用Python，你就不用担心学了会没用。</p>
<p>项目是Python3开发的，建议安装Python3。</p>
<p>Python官网下载地址：<a href="https://www.python.org/downloads/" target="_blank" rel="noopener">https://www.python.org/downloads/</a> ，该安装包有安装指引，按步骤完成即可。</p>
<p>详细步骤请参考文章<a href="https://baijiahao.baidu.com/s?id=1606573927720991570&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">新手必看！如何在windows下安装Python(Python入门教程)</a>。</p>
<h3 id="检测环境变量，若无按如下配置："><a href="#检测环境变量，若无按如下配置：" class="headerlink" title="检测环境变量，若无按如下配置："></a>检测环境变量，若无按如下配置：</h3><h4 id="1-打开环境变量配置"><a href="#1-打开环境变量配置" class="headerlink" title="1). 打开环境变量配置"></a>1). 打开环境变量配置</h4><p>计算机→属性→高级系统设置→高级→环境变量，在系统变量中配置。</p>
<h4 id="2-配置PYTHON-HOME"><a href="#2-配置PYTHON-HOME" class="headerlink" title="2). 配置PYTHON_HOME"></a>2). 配置PYTHON_HOME</h4><p>新建，变量名PYTHON_HOME，变量值，Python路径，我的路径是C:\my\Python，保存。</p>
<h4 id="3-配置Path"><a href="#3-配置Path" class="headerlink" title="3). 配置Path"></a>3). 配置Path</h4><p>打开Path变量，在变量值最前加入%PYTHON_HOME%;%PYTHON_HOME%\Scripts; （分号是分隔符，后面有其它值时必须有分号）。</p>
<p>python安装好之后，我们要检测一下是否安装成功，用系统管理员打开命令行工具cmd，输入“python -V”,然后敲回车，如果出现如下信息，则表示我们安装成功了。</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">C</span>:\<span class="selector-tag">Users</span>\<span class="selector-tag">mmm</span>&gt;<span class="selector-tag">python</span> <span class="selector-tag">-V</span></span><br><span class="line"><span class="selector-tag">Python</span> 3<span class="selector-class">.7</span><span class="selector-class">.4</span></span><br></pre></td></tr></table></figure>
<h2 id="5-IDE（PyCharm）"><a href="#5-IDE（PyCharm）" class="headerlink" title="5. IDE（PyCharm）"></a>5. IDE（PyCharm）</h2><p>PyCharm是一种Python IDE，带有一整套可以帮助用户在使用Python语言开发时提高其效率的工具，比如调试、语法高亮、Project管理、代码跳转、智能提示、自动完成、单元测试、版本控制。此外，该IDE提供了一些高级功能，以用于支持Django框架下的专业Web开发。</p>
<p>a. 官网Professional(专业版)下载地址：<a href="https://www.jetbrains.com/pycharm/download/#section=windows" target="_blank" rel="noopener">https://www.jetbrains.com/pycharm/download/#section=windows</a></p>
<p>b. 下载Jetbrains系列产品2019.2.2激活破解补丁jetbrains-agent.jar文件</p>
<p>网盘链接：<a href="https://pan.baidu.com/s/17Q5mpAT49scLdhVG5hvBgg" target="_blank" rel="noopener">https://pan.baidu.com/s/17Q5mpAT49scLdhVG5hvBgg</a> 提取码：k6zb</p>
<p>c. 参考<a href="https://www.jianshu.com/p/4e00d0aafc52/" target="_blank" rel="noopener">pycharm永久激活教程</a>，永久激活。</p>
<h2 id="6-Scrapy"><a href="#6-Scrapy" class="headerlink" title="6. Scrapy"></a>6. Scrapy</h2><p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。</p>
<p>Scrapy依赖的库比较多，在安装之前，你需要确保以下库已经安装：wheel、lxml、pyOpenSSL、Twisted、pywin32，先装完，再装Scrapy。使用可阅读文章<a href="https://www.jianshu.com/p/d598f6184455" target="_blank" rel="noopener">Scrapy入门简介及Demo</a>。<br>库地址：<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/" target="_blank" rel="noopener">https://www.lfd.uci.edu/~gohlke/pythonlibs/</a></p>
<h3 id="安装方式一："><a href="#安装方式一：" class="headerlink" title="安装方式一："></a>安装方式一：</h3><h4 id="安装wheel"><a href="#安装wheel" class="headerlink" title="安装wheel"></a>安装wheel</h4><h5 id="用途："><a href="#用途：" class="headerlink" title="用途："></a>用途：</h5><p>pip安装固然方便，但有时候会遇到安装失败的问题。wheel和egg都是打包的格式，支持不需要编译或制作的安装过程。wheel现在被认为是Python标准的二进制打包格式。</p>
<h5 id="安装命令："><a href="#安装命令：" class="headerlink" title="安装命令："></a>安装命令：</h5><p>pip install wheel</p>
<p><em>注意：如果你是刚刚安装过python并且从没有安装过wheel，你可以直接运行上述命令。但如果你的pip版本不够新，你需要在执行install命令之前更新一下pip，在命令行中输入：python -m pip install –upgrade pip更新pip，再输入安装命令即可。</em></p>
<h4 id="安装lxml"><a href="#安装lxml" class="headerlink" title="安装lxml"></a>安装lxml</h4><h5 id="用途：-1"><a href="#用途：-1" class="headerlink" title="用途："></a>用途：</h5><p>python的一个解析库,支持HTML和XML的解析,支持XPath解析方式,而且解析效率非常高。</p>
<h5 id="安装命令：-1"><a href="#安装命令：-1" class="headerlink" title="安装命令："></a>安装命令：</h5><h6 id="安装方式一-在线-："><a href="#安装方式一-在线-：" class="headerlink" title="安装方式一(在线)："></a>安装方式一(在线)：</h6><p>pip install lxml</p>
<h6 id="安装方式二（离线-："><a href="#安装方式二（离线-：" class="headerlink" title="安装方式二（离线)："></a>安装方式二（离线)：</h6><p>你可以进入地址<br>[<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml]" target="_blank" rel="noopener">https://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml]</a>:</p>
<p>去下载lxml，然后用安装.whl文件的方式安装。<br>pip install lxml‑4.4.1‑cp37‑cp37m‑win_amd64.whl</p>
<p><strong>下面安装都省略离线安装方式。</strong></p>
<h4 id="安装zope-interface"><a href="#安装zope-interface" class="headerlink" title="安装zope.interface"></a>安装zope.interface</h4><h5 id="用途：-2"><a href="#用途：-2" class="headerlink" title="用途："></a>用途：</h5><p>python本身不提供interface的实现,需要通过第三方扩展库来使用类似interface的功能,一般都是zope.interface。</p>
<p><em>注意：不安装zope.interface可能会出现pyOpenSSL安装失败。</em></p>
<h5 id="安装命令：-2"><a href="#安装命令：-2" class="headerlink" title="安装命令："></a>安装命令：</h5><p>pip install zope.interface</p>
<h4 id="安装pyOpenSSL"><a href="#安装pyOpenSSL" class="headerlink" title="安装pyOpenSSL"></a>安装pyOpenSSL</h4><h5 id="用途：-3"><a href="#用途：-3" class="headerlink" title="用途："></a>用途：</h5><p>让python支持SSL通信协议，简单来说就是加密解密等这系列操作。</p>
<h5 id="安装命令：-3"><a href="#安装命令：-3" class="headerlink" title="安装命令："></a>安装命令：</h5><p>pip install pyOpenSSL</p>
<h4 id="安装Twisted"><a href="#安装Twisted" class="headerlink" title="安装Twisted"></a>安装Twisted</h4><h5 id="用途：-4"><a href="#用途：-4" class="headerlink" title="用途："></a>用途：</h5><p>Twisted是用Python实现的基于事件驱动的网络引擎框架，Twisted支持许多常见的传输及应用层协议，包括TCP、UDP、SSL/TLS、HTTP、IMAP、SSH、IRC以及FTP。就像Python一样，Twisted也具有“内置电池”（batteries-included）的特点。Twisted对于其支持的所有协议都带有客户端和服务器实现，同时附带有基于命令行的工具，使得配置和部署产品级的Twisted应用变得非常方便。</p>
<p>详情参见[<a href="https://www.cnblogs.com/misswangxing/p/7712318.html]" target="_blank" rel="noopener">https://www.cnblogs.com/misswangxing/p/7712318.html]</a></p>
<h5 id="安装命令：-4"><a href="#安装命令：-4" class="headerlink" title="安装命令："></a>安装命令：</h5><p>pip install Twisted</p>
<h4 id="安装pywin32"><a href="#安装pywin32" class="headerlink" title="安装pywin32"></a>安装pywin32</h4><h5 id="用途：-5"><a href="#用途：-5" class="headerlink" title="用途："></a>用途：</h5><p>python不自带访问Windows API的库，需要下载这个库做支持。</p>
<h5 id="安装命令：-5"><a href="#安装命令：-5" class="headerlink" title="安装命令："></a>安装命令：</h5><p>pip install pywin32</p>
<p>#在python安装根路径的Scripts目录下执行<br>python pywin32_postinstall.py -install</p>
<h4 id="安装Scrapy"><a href="#安装Scrapy" class="headerlink" title="安装Scrapy"></a>安装Scrapy</h4><h5 id="安装命令：-6"><a href="#安装命令：-6" class="headerlink" title="安装命令："></a>安装命令：</h5><p>命令：pip install scrapy</p>
<h4 id="可能出现的问题"><a href="#可能出现的问题" class="headerlink" title="可能出现的问题"></a><strong>可能出现的问题</strong></h4><p>问题：You are using pip version 19.0.3, however version 19.X is available.<br>解决方法：输入命令python -m pip install -U pip 或 python -m pip install –upgrade pip </p>
<p>模块安装完毕，输入’scrapy -h’, 输出信息如下则表示Scrapy安装成功。</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\mmm&gt;scrapy -h</span><br><span class="line">Scrapy 1.7.3 - <span class="literal">no</span> active project</span><br><span class="line"></span><br><span class="line">Usage:</span><br><span class="line">  scrapy &lt;command&gt; [options] [args]</span><br><span class="line"></span><br><span class="line">Available commands:</span><br><span class="line">  bench         <span class="builtin-name">Run</span> quick benchmark test</span><br><span class="line">  fetch         Fetch a URL using the Scrapy downloader</span><br><span class="line">  genspider     Generate new spider using pre-defined templates</span><br><span class="line">  runspider     <span class="builtin-name">Run</span> a self-contained spider (without creating a project)</span><br><span class="line"> <span class="built_in"> settings </span>     <span class="builtin-name">Get</span><span class="built_in"> settings </span>values</span><br><span class="line">  shell         Interactive scraping console</span><br><span class="line">  startproject  Create new project</span><br><span class="line">  version       <span class="builtin-name">Print</span> Scrapy version</span><br><span class="line">  view          Open URL <span class="keyword">in</span> browser, as seen by Scrapy</span><br><span class="line"></span><br><span class="line">  [ more ]      More commands available when <span class="builtin-name">run</span> <span class="keyword">from</span> project directory</span><br></pre></td></tr></table></figure>
<h3 id="安装方式二：使用Anaconda工具，快速解决系列"><a href="#安装方式二：使用Anaconda工具，快速解决系列" class="headerlink" title="安装方式二：使用Anaconda工具，快速解决系列"></a>安装方式二：使用Anaconda工具，快速解决系列</h3><p>其实，你还可以登录Scrapy中文网，使用Anaconda进行安装，这个方式可能更适合初学编程的童鞋。地址如下：[ <a href="http://www.scrapyd.cn/doc/124.html" target="_blank" rel="noopener">http://www.scrapyd.cn/doc/124.html</a> ]</p>
<h3 id="验证安装是否成功"><a href="#验证安装是否成功" class="headerlink" title="验证安装是否成功"></a>验证安装是否成功</h3><h4 id="方法一："><a href="#方法一：" class="headerlink" title="方法一："></a>方法一：</h4><p>命令行：pip list</p>
<p>列表中出现了Scrapy，安装成功。</p>
<h4 id="方法二："><a href="#方法二：" class="headerlink" title="方法二："></a>方法二：</h4><p>命令行：scrapy</p>
<p>正确执行命令，安装成功。</p>
<h3 id="文件的功能"><a href="#文件的功能" class="headerlink" title="文件的功能"></a>文件的功能</h3><p>scrapy.cfg：配置文件</p>
<p>spiders：存放你Spider文件，也就是你爬取的py文件</p>
<p>items.py：相当于一个容器，和字典较像</p>
<p>middlewares.py：定义Downloader Middlewares(下载器中间件)和Spider Middlewares(蜘蛛中间件)的实现</p>
<p>pipelines.py:定义Item Pipeline的实现，实现数据的清洗，储存，验证。</p>
<p>settings.py：全局配置</p>
<h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><h4 id="a-创建工程结构"><a href="#a-创建工程结构" class="headerlink" title="a.  创建工程结构"></a>a.  创建工程结构</h4><p>cmd中执行命令：scrapy startproject  工程名</p>
<p><strong>快速打开当前文件夹的dos命令窗口：找到指定文件的文件夹，按住“shift+右键”，选择“在此处打开命令窗口”或“在此处打开Powershell窗口”即可。</strong></p>
<figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PS C:<span class="symbol">\d</span><span class="symbol">\m</span>mm<span class="symbol">\p</span>ycharm&gt; scrapy startproject baidu_baike</span><br><span class="line">New Scrapy project 'baidu_baike', using template directory 'c:<span class="symbol">\m</span>y<span class="symbol">\p</span>ython<span class="symbol">\l</span>ib<span class="symbol">\s</span>ite-packages<span class="symbol">\s</span>crapy<span class="symbol">\t</span>emplates<span class="symbol">\p</span>roject', created in:</span><br><span class="line">    C:<span class="symbol">\d</span><span class="symbol">\m</span>mm<span class="symbol">\p</span>ycharm<span class="symbol">\b</span>aidu_baike</span><br><span class="line"></span><br><span class="line">You can start your first spider with:</span><br><span class="line">    cd baidu_baike</span><br><span class="line">    scrapy genspider example example.com</span><br></pre></td></tr></table></figure>
<h4 id="b-创建一个spider（爬虫文件）"><a href="#b-创建一个spider（爬虫文件）" class="headerlink" title="b.  创建一个spider（爬虫文件）"></a>b.  创建一个spider（爬虫文件）</h4><p>cmd中执行命令：scrapy genspider 文件名 要爬取的网址。</p>
<p>在spiders文件夹下创建</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PS C:\d\mmm\pycharm\baidu_baike\baidu_baike\spiders&gt; scrapy genspider -t basic baidubaike baike<span class="selector-class">.baidu</span><span class="selector-class">.com</span></span><br><span class="line">Created spider <span class="string">'baidubaike'</span> using template <span class="string">'basic'</span> <span class="keyword">in</span> module:</span><br><span class="line">  baidu_baike<span class="selector-class">.spiders</span><span class="selector-class">.baidubaike</span></span><br></pre></td></tr></table></figure>
<p>生成baidubaike.py的代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaidubaikeSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'baidu'</span></span><br><span class="line">    allowed_domains = [<span class="string">'baike.baidu.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://baike.baidu.com/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>name：是项目的名字</p>
<p>allowed_domains：是允许爬取的域名，比如一些网站有相关链接，域名就和本网站不同，这些就会忽略。</p>
<p>atart_urls：是Spider爬取的网站，定义初始的请求url，可以多个。</p>
<p>parse方法：是Spider的一个方法，在请求start_url后，之后的方法，这个方法是对网页的解析，与提取自己想要的东西。</p>
<p>response参数：是请求网页后返回的内容，也就是你需要解析的网页。</p>
<h4 id="c-运行"><a href="#c-运行" class="headerlink" title="c.  运行"></a>c.  运行</h4><p>cmd中执行命令：scrapy crawl 项目名</p>
<p>在项目根路径下执行</p>
<figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PS C:\d\mmm\pycharm\baidu_baike&gt; scrapy crawl baidu</span><br><span class="line"><span class="number">2019</span>-<span class="number">09</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">45</span>:<span class="number">40</span> [scrapy.utils.log] INFO: Scrapy <span class="number">1.7</span><span class="meta">.3</span> started (bot: baidu_baike)</span><br><span class="line"><span class="number">2019</span>-<span class="number">09</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">45</span>:<span class="number">40</span> [scrapy.utils.log] INFO: Versions: lxml <span class="number">4.4</span><span class="meta">.1</span><span class="meta">.0</span>, libxml2 <span class="number">2.9</span><span class="meta">.5</span>, cssselect <span class="number">1.1</span><span class="meta">.0</span>, parsel <span class="number">1.5</span><span class="meta">.2</span>, w3lib <span class="number">1.21</span><span class="meta">.0</span>, Twisted <span class="number">19.7</span><span class="meta">.0</span>, Python <span class="number">3.7</span><span class="meta">.4</span> (tags/v3<span class="meta">.7</span><span class="meta">.4</span>:e09359112e, Jul  <span class="number">8</span> <span class="number">2019</span>, <span class="number">20</span>:<span class="number">34</span>:<span class="number">20</span>) [MSC v<span class="meta">.1916</span> <span class="number">64</span> bit (AMD64)], pyOpenSSL <span class="number">19.0</span><span class="meta">.0</span> (OpenSSL <span class="number">1.1</span>.1c  <span class="number">28</span> May <span class="number">2019</span>), cryptography <span class="number">2.7</span>, Platform Windows-<span class="number">10</span>-<span class="number">10.0</span><span class="meta">.18362</span>-SP0</span><br></pre></td></tr></table></figure>
<h1 id="三-项目构建"><a href="#三-项目构建" class="headerlink" title="三. 项目构建"></a>三. 项目构建</h1><h2 id="1-Mysql建库"><a href="#1-Mysql建库" class="headerlink" title="1. Mysql建库"></a>1. Mysql建库</h2><p>库内包含 演员、电影、电影类型、演员-&gt;电影、电影-&gt;类型 五张表：</p>
<ul>
<li>演员 ：爬取内容为 ID, 简介， 中文名，外文名，国籍，星座，出生地，出生日期，代表作品，主要成就，经纪公司；</li>
<li>电影 ：ID，简介，中文名，外文名，出品时间，出品公司，导演，编剧，类型，主演，片长，上映时间，对白语言，主要成就；</li>
<li>电影类型： 爱情，喜剧，动作，剧情，科幻，恐怖，动画，惊悚，犯罪，冒险，其他；</li>
<li>演员-&gt;电影： 演员ID， 电影ID；</li>
<li>电影-&gt; 类型： 电影ID， 类型ID；</li>
</ul>
<p>与其相对应的建表语句即要求请参考craw_without_spider/mysql/creat_sql.txt文件。 在修改目标库的名称后，即可使用工具MySQL Workbench执行creat_sql.txt创建数据库。</p>
<h2 id="2-百度百科爬虫"><a href="#2-百度百科爬虫" class="headerlink" title="2. 百度百科爬虫"></a>2. 百度百科爬虫</h2><p>该爬虫对应与crawl 下的baidu_baike 文件夹。该爬虫基于scrapy框架，爬取电影类数据，包含电影22219部，演员13967人，演员电影间联系1942个，电影与类别间联系23238,其中类别为‘其他’的电影有10个。对应数据集可在<a href="https://pan.baidu.com/s/1wrqvxfT-S4NhGgnTJT4Ltg" target="_blank" rel="noopener">百度云下载</a>，提取码：60n6。</p>
<h3 id="修改item-py"><a href="#修改item-py" class="headerlink" title="修改item.py"></a>修改item.py</h3><p>在安装scrapy 后，可以通过 scrapy startproject baidu_baike 初始化爬虫框架，它的目录结构为：</p>
<p>.:<br>baidu_baike scrapy.cfg<br>./baidu_baike:<br><strong>init</strong>.py items.py middlewares.py pipelines.py settings.py spiders<br><strong>init</strong>.pyc items.pyc middlewares.pyc pipelines.pyc settings.pyc<br>./baidu_baike/spiders:<br>baidu_baike.py baidu_baike.pyc <strong>init</strong>.py <strong>init</strong>.pyc</p>
<p>baidu_baike/目录下的文件是需要我们手动修改的。其中 items.py 对需要爬取的内容进行管理，便于把抓取的内容传递进pipelines进行后期处理。现在我们对 <code>baidu_baike/baidu_baike/item.py</code>进行修改，添加要爬取的项。</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">BaiduBaikeItem</span>(<span class="title">scrapy</span>.<span class="type">Item</span>):</span></span><br><span class="line"><span class="class">    # define the fields for your item here like:     # name = scrapy.<span class="type">Field</span>()     # 包含演员相关属性     actor_id = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    actor_bio = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    actor_chName = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    actor_foreName = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    actor_nationality = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    actor_constellation = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    actor_birthPlace = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    actor_birthDay = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    actor_repWorks = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    actor_achiem = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    actor_brokerage = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    # 电影相关属性 </span></span><br><span class="line"><span class="class">    movie_id = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    movie_bio = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    movie_chName = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    movie_foreName = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    movie_prodTime = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    movie_prodCompany = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    movie_director = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    movie_screenwriter = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    movie_genre = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    movie_star = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    movie_length = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    movie_rekeaseTime = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    movie_language = scrapy.<span class="type">Field</span>()</span></span><br><span class="line"><span class="class">    movie_achiem = scrapy.<span class="type">Field</span>()</span></span><br></pre></td></tr></table></figure>
<p>在爬虫运行过程中，我们主要爬取电影和演员两类及其对应的各项属性。对于电影-&gt;类别 和 演员-&gt;电影两个表会在爬取数据后进行建立。</p>
<h3 id="修改-pipelines-py"><a href="#修改-pipelines-py" class="headerlink" title="修改 pipelines.py"></a>修改 pipelines.py</h3><p>pipelines.py 用来将爬取的内容存放到MySQL数据库中。类内有初始化<strong>init</strong>()、处理爬取内容并保存process_item()、关闭数据库close_spider()三个方法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># Define your item pipelines here</span><br><span class="line">#</span><br><span class="line"># Don&apos;t forget to add your pipeline to the ITEM_PIPELINES setting</span><br><span class="line"># See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html</span><br><span class="line">from __future__ import absolute_import</span><br><span class="line">from __future__ import division</span><br><span class="line">from __future__ import print_function</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import pymysql</span><br><span class="line">from pymysql import connections</span><br><span class="line">from baidu_baike import settings</span><br><span class="line"></span><br><span class="line">class BaiduBaikePipeline(object):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        # 初始化并连接到mysql数据库</span><br><span class="line">        self.conn = pymysql.connect(</span><br><span class="line">            host=settings.HOST_IP,</span><br><span class="line">            port=settings.PORT,</span><br><span class="line">            user=settings.USER,</span><br><span class="line">            passwd=settings.PASSWD,</span><br><span class="line">            db=settings.DB_NAME,</span><br><span class="line">            charset=&apos;utf8mb4&apos;,</span><br><span class="line">            use_unicode=True</span><br><span class="line">            )</span><br><span class="line">        self.cursor = self.conn.cursor()</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        # process info for actor</span><br><span class="line">        actor_chName = str(item[&apos;actor_chName&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line">        actor_foreName = str(item[&apos;actor_foreName&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line">        movie_chName = str(item[&apos;movie_chName&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line">        movie_foreName = str(item[&apos;movie_foreName&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line"></span><br><span class="line">        if (item[&apos;actor_chName&apos;] != None or item[&apos;actor_foreName&apos;] != None) and item[&apos;movie_chName&apos;] == None:</span><br><span class="line">            actor_bio = str(item[&apos;actor_bio&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line">            actor_nationality = str(item[&apos;actor_nationality&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line">            actor_constellation = str(item[&apos;actor_constellation&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line">            actor_birthPlace = str(item[&apos;actor_birthPlace&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line">            actor_birthDay = str(item[&apos;actor_birthDay&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line">            actor_repWorks = str(item[&apos;actor_repWorks&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line">            actor_achiem = str(item[&apos;actor_achiem&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line">            actor_brokerage = str(item[&apos;actor_brokerage&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line"></span><br><span class="line">            self.cursor.execute(&quot;SELECT actor_chName FROM actor;&quot;)</span><br><span class="line">            actorList = self.cursor.fetchall()</span><br><span class="line">            if (actor_chName,) not in actorList :</span><br><span class="line">                # get the nums of actor_id in table actor</span><br><span class="line">                self.cursor.execute(&quot;SELECT MAX(actor_id) FROM actor&quot;)</span><br><span class="line">                result = self.cursor.fetchall()[0]</span><br><span class="line">                if None in result:</span><br><span class="line">                    actor_id = 1</span><br><span class="line">                else:</span><br><span class="line">                    actor_id = result[0] + 1</span><br><span class="line">                sql = &quot;&quot;&quot;</span><br><span class="line">                INSERT INTO actor(actor_id, actor_bio, actor_chName, actor_foreName, actor_nationality, actor_constellation, actor_birthPlace, actor_birthDay, actor_repWorks, actor_achiem, actor_brokerage ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)</span><br><span class="line">                &quot;&quot;&quot;</span><br><span class="line">                self.cursor.execute(sql, (actor_id, actor_bio, actor_chName, actor_foreName, actor_nationality, actor_constellation, actor_birthPlace, actor_birthDay, actor_repWorks, actor_achiem, actor_brokerage ))</span><br><span class="line">                self.conn.commit()</span><br><span class="line">            else:</span><br><span class="line">                print(&quot;#&quot; * 20, &quot;Got a duplict actor!!&quot;, actor_chName)</span><br><span class="line">        elif (item[&apos;movie_chName&apos;] != None or item[&apos;movie_foreName&apos;] != None) and item[&apos;actor_chName&apos;] == None:</span><br><span class="line">            movie_bio = str(item[&apos;movie_bio&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line">            movie_prodTime = str(item[&apos;movie_prodTime&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line">            movie_prodCompany = str(item[&apos;movie_prodCompany&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line">            movie_director = str(item[&apos;movie_director&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line">            movie_screenwriter = str(item[&apos;movie_screenwriter&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line">            movie_genre = str(item[&apos;movie_genre&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line">            movie_star = str(item[&apos;movie_star&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line">            movie_length = str(item[&apos;movie_length&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line">            movie_rekeaseTime = str(item[&apos;movie_rekeaseTime&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line">            movie_language = str(item[&apos;movie_language&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line">            movie_achiem = str(item[&apos;movie_achiem&apos;]).encode(&apos;utf-8&apos;)</span><br><span class="line"></span><br><span class="line">            self.cursor.execute(&quot;SELECT movie_chName FROM movie;&quot;)</span><br><span class="line">            movieList = self.cursor.fetchall()</span><br><span class="line">            if (movie_chName,) not in movieList :</span><br><span class="line">                self.cursor.execute(&quot;SELECT MAX(movie_id) FROM movie&quot;)</span><br><span class="line">                result = self.cursor.fetchall()[0]</span><br><span class="line">                if None in result:</span><br><span class="line">                    movie_id = 1</span><br><span class="line">                else:</span><br><span class="line">                    movie_id = result[0] + 1</span><br><span class="line">                sql = &quot;&quot;&quot;</span><br><span class="line">                INSERT INTO movie(  movie_id, movie_bio, movie_chName, movie_foreName, movie_prodTime, movie_prodCompany, movie_director, movie_screenwriter, movie_genre, movie_star, movie_length, movie_rekeaseTime, movie_language, movie_achiem ) VALUES ( %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)</span><br><span class="line">                &quot;&quot;&quot;</span><br><span class="line">                self.cursor.execute(sql, ( movie_id, movie_bio, movie_chName, movie_foreName, movie_prodTime, movie_prodCompany, movie_director, movie_screenwriter, movie_genre, movie_star, movie_length, movie_rekeaseTime, movie_language, movie_achiem ))</span><br><span class="line">                self.conn.commit()</span><br><span class="line">            else:</span><br><span class="line">                print(&quot;Got a duplict movie!!&quot;, movie_chName)</span><br><span class="line">        else:</span><br><span class="line">            print(&quot;Skip this page because wrong category!! &quot;)</span><br><span class="line">        return item</span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        self.conn.close()</span><br></pre></td></tr></table></figure>
<h3 id="修改中间件-middlewares-py"><a href="#修改中间件-middlewares-py" class="headerlink" title="修改中间件 middlewares.py"></a>修改中间件 middlewares.py</h3><p>middlewares.py 内包含一些UserAgent 和 代理来防止被封。可以自己搜集把自带的替换掉，也可以直接用我项目内的。</p>
<h3 id="修改-settings-py"><a href="#修改-settings-py" class="headerlink" title="修改 settings.py"></a>修改 settings.py</h3><p>settings.py 包含了爬虫相关的设置，通常我们需要修改我们自定义的pipelines、中间件、随机延迟等信息。这里只需要注意的是，使用爬虫时最好设置一些延迟，尤其目标网站较小时。</p>
<h3 id="编写-baidu-baike-py"><a href="#编写-baidu-baike-py" class="headerlink" title="编写 baidu_baike.py"></a>编写 baidu_baike.py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> baidu_baike.items <span class="keyword">import</span> BaiduBaikeItem</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaiduBaikeSpider</span><span class="params">(scrapy.Spider, object)</span>:</span></span><br><span class="line">    <span class="comment"># 定义爬虫名称</span></span><br><span class="line">    name = <span class="string">'baidu'</span></span><br><span class="line">    <span class="comment"># 设置允许的域，不以这个开头的链接不会爬取</span></span><br><span class="line">    allowed_domains = [<span class="string">"baike.baidu.com"</span>]</span><br><span class="line">    <span class="comment"># 爬虫开始的的网址</span></span><br><span class="line">    start_urls = [<span class="string">'https://baike.baidu.com/item/%E5%88%98%E5%BE%B7%E5%8D%8E/114923'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#start_urls = ['https://baike.baidu.com/item/%E4%B8%83%E5%B0%8F%E7%A6%8F']</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将返回的标签列表提取文本并返回</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_from_findall</span><span class="params">(self, tag_list)</span>:</span></span><br><span class="line">        result = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> slist <span class="keyword">in</span> tag_list:</span><br><span class="line">            tmp = slist.get_text()</span><br><span class="line">            result.append(tmp)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 程序的核心，可以获取页面内的指定信息，并获取页面内的所有链接做进一步的爬取</span></span><br><span class="line">    <span class="comment"># response 是初始网址的返回</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 分析 response来提取出页面最下部的标签信息，如果包含演员或电影则进行爬取，否则跳过</span></span><br><span class="line">        page_category = response.xpath(<span class="string">"//dd[@id='open-tag-item']/span[@class='taglist']/text()"</span>).extract()</span><br><span class="line">        page_category = [l.strip() <span class="keyword">for</span> l <span class="keyword">in</span> page_category]</span><br><span class="line">        item = BaiduBaikeItem()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># tooooo ugly,,,, but can not use defaultdict</span></span><br><span class="line">        <span class="keyword">for</span> sub_item <span class="keyword">in</span> [<span class="string">'actor_bio'</span>, <span class="string">'actor_chName'</span>, <span class="string">'actor_foreName'</span>, <span class="string">'actor_nationality'</span>, <span class="string">'actor_constellation'</span>,</span><br><span class="line">                         <span class="string">'actor_birthPlace'</span>, <span class="string">'actor_birthDay'</span>, <span class="string">'actor_repWorks'</span>, <span class="string">'actor_achiem'</span>, <span class="string">'actor_brokerage'</span>,</span><br><span class="line">                         <span class="string">'movie_bio'</span>, <span class="string">'movie_chName'</span>, <span class="string">'movie_foreName'</span>, <span class="string">'movie_prodTime'</span>, <span class="string">'movie_prodCompany'</span>,</span><br><span class="line">                         <span class="string">'movie_director'</span>, <span class="string">'movie_screenwriter'</span>, <span class="string">'movie_genre'</span>, <span class="string">'movie_star'</span>, <span class="string">'movie_length'</span>,</span><br><span class="line">                         <span class="string">'movie_rekeaseTime'</span>, <span class="string">'movie_language'</span>, <span class="string">'movie_achiem'</span>]:</span><br><span class="line">            item[sub_item] = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果包含演员标签则认为是演员</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">u'演员'</span> <span class="keyword">in</span> page_category:</span><br><span class="line">            print(<span class="string">"Get a actor page"</span>)</span><br><span class="line">            soup = BeautifulSoup(response.text, <span class="string">'lxml'</span>)</span><br><span class="line">            summary_node = soup.find(<span class="string">"div"</span>, class_=<span class="string">"lemma-summary"</span>)</span><br><span class="line">            item[<span class="string">'actor_bio'</span>] = summary_node.get_text().replace(<span class="string">"\n"</span>, <span class="string">" "</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 使用 bs4 对页面内信息进行提取并保存到对应的item内</span></span><br><span class="line">            all_basicInfo_Item = soup.find_all(<span class="string">"dt"</span>, class_=<span class="string">"basicInfo-item name"</span>)</span><br><span class="line">            basic_item = self._get_from_findall(all_basicInfo_Item)</span><br><span class="line">            basic_item = [s.strip() <span class="keyword">for</span> s <span class="keyword">in</span> basic_item]</span><br><span class="line">            all_basicInfo_value = soup.find_all(<span class="string">"dd"</span>, class_=<span class="string">"basicInfo-item value"</span>)</span><br><span class="line">            basic_value = self._get_from_findall(all_basicInfo_value)</span><br><span class="line">            basic_value = [s.strip() <span class="keyword">for</span> s <span class="keyword">in</span> basic_value]</span><br><span class="line">            <span class="keyword">for</span> i, info <span class="keyword">in</span> enumerate(basic_item):</span><br><span class="line">                info = info.replace(<span class="string">u"\xa0"</span>, <span class="string">""</span>)</span><br><span class="line">                <span class="keyword">if</span> info == <span class="string">u'中文名'</span>:</span><br><span class="line">                    item[<span class="string">'actor_chName'</span>] = basic_value[i]</span><br><span class="line">                <span class="keyword">elif</span> info == <span class="string">u'外文名'</span>:</span><br><span class="line">                    item[<span class="string">'actor_foreName'</span>] = basic_value[i]</span><br><span class="line">                <span class="keyword">elif</span> info == <span class="string">u'国籍'</span>:</span><br><span class="line">                    item[<span class="string">'actor_nationality'</span>] = basic_value[i]</span><br><span class="line">                <span class="keyword">elif</span> info == <span class="string">u'星座'</span>:</span><br><span class="line">                    item[<span class="string">'actor_constellation'</span>] = basic_value[i]</span><br><span class="line">                <span class="keyword">elif</span> info == <span class="string">u'出生地'</span>:</span><br><span class="line">                    item[<span class="string">'actor_birthPlace'</span>] = basic_value[i]</span><br><span class="line">                <span class="keyword">elif</span> info == <span class="string">u'出生日期'</span>:</span><br><span class="line">                    item[<span class="string">'actor_birthDay'</span>] = basic_value[i]</span><br><span class="line">                <span class="keyword">elif</span> info == <span class="string">u'代表作品'</span>:</span><br><span class="line">                    item[<span class="string">'actor_repWorks'</span>] = basic_value[i]</span><br><span class="line">                <span class="keyword">elif</span> info == <span class="string">u'主要成就'</span>:</span><br><span class="line">                    item[<span class="string">'actor_achiem'</span>] = basic_value[i]</span><br><span class="line">                <span class="keyword">elif</span> info == <span class="string">u'经纪公司'</span>:</span><br><span class="line">                    item[<span class="string">'actor_brokerage'</span>] = basic_value[i]</span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line">        <span class="keyword">elif</span> <span class="string">u'电影'</span> <span class="keyword">in</span> page_category:</span><br><span class="line">            print(<span class="string">"Get a movie page!!"</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 使用 bs4 对页面内的链接进行提取，而后进行循环爬取</span></span><br><span class="line">            soup = BeautifulSoup(response.text, <span class="string">'lxml'</span>)</span><br><span class="line">            summary_node = soup.find(<span class="string">"div"</span>, class_=<span class="string">"lemma-summary"</span>)</span><br><span class="line">            item[<span class="string">'movie_bio'</span>] = summary_node.get_text().replace(<span class="string">"\n"</span>, <span class="string">" "</span>)</span><br><span class="line">            all_basicInfo_Item = soup.find_all(<span class="string">"dt"</span>, class_=<span class="string">"basicInfo-item name"</span>)</span><br><span class="line">            basic_item = self._get_from_findall(all_basicInfo_Item)</span><br><span class="line">            basic_item = [s.strip() <span class="keyword">for</span> s <span class="keyword">in</span> basic_item]</span><br><span class="line">            all_basicInfo_value = soup.find_all(<span class="string">"dd"</span>, class_=<span class="string">"basicInfo-item value"</span>)</span><br><span class="line">            basic_value = self._get_from_findall(all_basicInfo_value)</span><br><span class="line">            basic_value = [s.strip() <span class="keyword">for</span> s <span class="keyword">in</span> basic_value]</span><br><span class="line">            <span class="keyword">for</span> i, info <span class="keyword">in</span> enumerate(basic_item):</span><br><span class="line">                info = info.replace(<span class="string">u"\xa0"</span>, <span class="string">""</span>)</span><br><span class="line">                <span class="keyword">if</span> info == <span class="string">u'中文名'</span>:</span><br><span class="line">                    item[<span class="string">'movie_chName'</span>] = basic_value[i]</span><br><span class="line">                <span class="keyword">elif</span> info == <span class="string">u'外文名'</span>:</span><br><span class="line">                    item[<span class="string">'movie_foreName'</span>] = basic_value[i]</span><br><span class="line">                <span class="keyword">elif</span> info == <span class="string">u'出品时间'</span>:</span><br><span class="line">                    item[<span class="string">'movie_prodTime'</span>] = basic_value[i]</span><br><span class="line">                <span class="keyword">elif</span> info == <span class="string">u'出品公司'</span>:</span><br><span class="line">                    item[<span class="string">'movie_prodCompany'</span>] = basic_value[i]</span><br><span class="line">                <span class="keyword">elif</span> info == <span class="string">u'导演'</span>:</span><br><span class="line">                    item[<span class="string">'movie_director'</span>] = basic_value[i]</span><br><span class="line">                <span class="keyword">elif</span> info == <span class="string">u'编剧'</span>:</span><br><span class="line">                    item[<span class="string">'movie_screenwriter'</span>] = basic_value[i]</span><br><span class="line">                <span class="keyword">elif</span> info == <span class="string">u'类型'</span>:</span><br><span class="line">                    item[<span class="string">'movie_genre'</span>] = basic_value[i]</span><br><span class="line">                <span class="keyword">elif</span> info == <span class="string">u'主演'</span>:</span><br><span class="line">                    item[<span class="string">'movie_star'</span>] = basic_value[i]</span><br><span class="line">                <span class="keyword">elif</span> info == <span class="string">u'片长'</span>:</span><br><span class="line">                    item[<span class="string">'movie_length'</span>] = basic_value[i]</span><br><span class="line">                <span class="keyword">elif</span> info == <span class="string">u'上映时间'</span>:</span><br><span class="line">                    item[<span class="string">'movie_rekeaseTime'</span>] = basic_value[i]</span><br><span class="line">                <span class="keyword">elif</span> info == <span class="string">u'对白语言'</span>:</span><br><span class="line">                    item[<span class="string">'movie_language'</span>] = basic_value[i]</span><br><span class="line">                <span class="keyword">elif</span> info == <span class="string">u'主要成就'</span>:</span><br><span class="line">                    item[<span class="string">'movie_achiem'</span>] = basic_value[i]</span><br><span class="line">            <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line">        soup = BeautifulSoup(response.text, <span class="string">'lxml'</span>)</span><br><span class="line">        links = soup.find_all(<span class="string">'a'</span>, href=re.compile(<span class="string">r"/item/"</span>))</span><br><span class="line">        <span class="keyword">for</span> link <span class="keyword">in</span> links:</span><br><span class="line">            new_url = link[<span class="string">"href"</span>]</span><br><span class="line">            new_full_url = urllib.parse.urljoin(<span class="string">'https://baike.baidu.com/'</span>, new_url)</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(new_full_url, callback=self.parse)</span><br></pre></td></tr></table></figure>
<h3 id="运行项目"><a href="#运行项目" class="headerlink" title="运行项目"></a>运行项目</h3><p>cmd中执行命令：scrapy crawl 项目名(baidu_baike.py中的name值)</p>
<p>在项目根路径下执行<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">PS C:\d\mmm\pycharm\baidu_baike&gt; scrapy crawl baidu</span><br><span class="line"><span class="number">2019</span>-<span class="number">09</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">45</span>:<span class="number">40</span> [scrapy<span class="selector-class">.utils</span><span class="selector-class">.log</span>] INFO: Scrapy <span class="number">1.7</span>.<span class="number">3</span> started (bot: baidu_baike)</span><br><span class="line"><span class="number">2019</span>-<span class="number">09</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">45</span>:<span class="number">40</span> [scrapy<span class="selector-class">.utils</span><span class="selector-class">.log</span>] INFO: Versions: lxml <span class="number">4.4</span>.<span class="number">1.0</span>, libxml2 <span class="number">2.9</span>.<span class="number">5</span>, cssselect <span class="number">1.1</span>.<span class="number">0</span>, parsel <span class="number">1.5</span>.<span class="number">2</span>, w3lib <span class="number">1.21</span>.<span class="number">0</span>, Twisted <span class="number">19.7</span>.<span class="number">0</span>, Python <span class="number">3.7</span>.<span class="number">4</span> (tags/v3.<span class="number">7.4</span>:e09359112e, Jul  <span class="number">8</span> <span class="number">2019</span>, <span class="number">20</span>:<span class="number">34</span>:<span class="number">20</span>) [MSC v.<span class="number">1916</span> <span class="number">64</span> bit (AMD64)], pyOpenSSL <span class="number">19.0</span>.<span class="number">0</span> (OpenSSL <span class="number">1.1</span>.<span class="number">1</span>c  <span class="number">28</span> May <span class="number">2019</span>), cryptography <span class="number">2.7</span>, Platform Windows-<span class="number">10</span>-<span class="number">10.0</span>.<span class="number">18362</span>-SP0</span><br><span class="line"><span class="number">2019</span>-<span class="number">09</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">45</span>:<span class="number">40</span> [scrapy.crawler] INFO: Overridden settings: &#123;<span class="string">'BOT_NAME'</span>: <span class="string">'baidu_baike'</span>, <span class="string">'NEWSPIDER_MODULE'</span>: <span class="string">'baidu_baike.spiders'</span>, <span class="string">'SPIDER_MODULES'</span>: [<span class="string">'baidu_baike.spiders'</span>], <span class="string">'USER_AGENT'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'</span>&#125;</span><br><span class="line"><span class="number">2019</span>-<span class="number">09</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">45</span>:<span class="number">40</span> [scrapy<span class="selector-class">.extensions</span><span class="selector-class">.telnet</span>] INFO: Telnet Password: <span class="number">4</span>e0e039c01ca23b2</span><br><span class="line"><span class="number">2019</span>-<span class="number">09</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">45</span>:<span class="number">40</span> [scrapy.middleware] INFO: Enabled extensions:</span><br><span class="line">[<span class="string">'scrapy.extensions.corestats.CoreStats'</span>,</span><br><span class="line"> <span class="string">'scrapy.extensions.telnet.TelnetConsole'</span>,</span><br><span class="line"> <span class="string">'scrapy.extensions.logstats.LogStats'</span>]</span><br><span class="line"><span class="number">2019</span>-<span class="number">09</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">45</span>:<span class="number">41</span> [scrapy.middleware] INFO: Enabled downloader middlewares:</span><br><span class="line">[<span class="string">'baidu_baike.middlewares.RandomUserAgent'</span>,</span><br><span class="line"> <span class="string">'baidu_baike.middlewares.ProxyMiddleWare'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.retry.RetryMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.cookies.CookiesMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.downloadermiddlewares.stats.DownloaderStats'</span>]</span><br><span class="line"><span class="number">2019</span>-<span class="number">09</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">45</span>:<span class="number">41</span> [scrapy.middleware] INFO: Enabled spider middlewares:</span><br><span class="line">[<span class="string">'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.offsite.OffsiteMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.referer.RefererMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware'</span>,</span><br><span class="line"> <span class="string">'scrapy.spidermiddlewares.depth.DepthMiddleware'</span>]</span><br><span class="line"><span class="number">2019</span>-<span class="number">09</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">45</span>:<span class="number">41</span> [scrapy.middleware] INFO: Enabled item pipelines:</span><br><span class="line">[<span class="string">'baidu_baike.pipelines.BaiduBaikePipeline'</span>]</span><br><span class="line"><span class="number">2019</span>-<span class="number">09</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">45</span>:<span class="number">41</span> [scrapy<span class="selector-class">.core</span><span class="selector-class">.engine</span>] INFO: Spider opened</span><br><span class="line"><span class="number">2019</span>-<span class="number">09</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">45</span>:<span class="number">41</span> [scrapy<span class="selector-class">.extensions</span><span class="selector-class">.logstats</span>] INFO: Crawled <span class="number">0</span> pages (at <span class="number">0</span> pages/min), scraped <span class="number">0</span> items (at <span class="number">0</span> items/min)</span><br><span class="line"><span class="number">2019</span>-<span class="number">09</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">45</span>:<span class="number">41</span> [scrapy<span class="selector-class">.extensions</span><span class="selector-class">.telnet</span>] INFO: Telnet console listening on <span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">6023</span></span><br><span class="line"><span class="number">2019</span>-<span class="number">09</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">45</span>:<span class="number">41</span> [scrapy<span class="selector-class">.core</span><span class="selector-class">.engine</span>] DEBUG: Crawled (<span class="number">200</span>) &lt;GET https:<span class="comment">//baike.baidu.com/item/%E5%88%98%E5%BE%B7%E5%8D%8E/114923&gt; (referer: None)</span></span><br><span class="line">Get <span class="selector-tag">a</span> actor page</span><br><span class="line"><span class="number">2019</span>-<span class="number">09</span>-<span class="number">26</span> <span class="number">09</span>:<span class="number">45</span>:<span class="number">42</span> [scrapy<span class="selector-class">.core</span><span class="selector-class">.scraper</span>] DEBUG: Scraped from &lt;<span class="number">200</span> https:<span class="comment">//baike.baidu.com/item/%E5%88%98%E5%BE%B7%E5%8D%8E/114923&gt;</span></span><br><span class="line">......</span><br></pre></td></tr></table></figure></p>
<h2 id="3-互动百科爬虫"><a href="#3-互动百科爬虫" class="headerlink" title="3. 互动百科爬虫"></a>3. 互动百科爬虫</h2><p>该爬虫对应与crawl 下的hudong_baike 文件夹。该爬虫基于scrapy框架，爬取电影类数据，包含电影13866部，演员5931人，演员电影间联系800个，电影与类别间联系14558,其中类别为‘其他’的电影有0个。对应数据集可在<a href="https://pan.baidu.com/s/1QBHfJzf4ETF2_UmqWSWBlQ" target="_blank" rel="noopener">百度云下载</a>，提取码：pya9 </p>
<p>互动百科爬虫的结构和百度百科相同。二者的主要不同之处在于二者的 info box 的格式不一致，因此采用了不同的方法进行提取。此处不再赘述。</p>
<h1 id="四-总结"><a href="#四-总结" class="headerlink" title="四. 总结"></a>四. 总结</h1><p>本文章对半结构化数据，即百度百科和互动百科做了爬取并保存到数据库中。这样就相当于我们获得了一份结构化的数据。下篇文章将使用直接映射和D2RQ将其转化为三元组的形式。</p>

    </div>
    
    <div class="post-footer">
        <div>
            
                转载声明：商业转载请联系作者获得授权,非商业转载请注明出处 © <a href="/" target="_blank">Ljjyy.com</a>
            
        </div>
        <div>
            
        </div>
    </div>
</article>

<div class="article-nav prev-next-wrap clearfix">
    
        <a href="/archives/2019/10/100592.html" class="pre-post btn btn-default" title='从零开始学习知识图谱 之 二.电影知识图谱构建 2.结构化数据到RDF以及基于Apache jena交互'>
            <i class="fa fa-angle-left fa-fw"></i><span class="hidden-lg">上一篇</span>
            <span class="hidden-xs">从零开始学习知识图谱 之 二.电影知识图谱构建 2.结构化数据到RDF以及基于Apache jena交互</span>
        </a>
    
    
        <a href="/archives/2019/10/100590.html" class="next-post btn btn-default" title='VirtualBox虚拟机安装Ubuntu'>
            <span class="hidden-lg">下一篇</span>
            <span class="hidden-xs">VirtualBox虚拟机安装Ubuntu</span><i class="fa fa-angle-right fa-fw"></i>
        </a>
    
</div>


    <div id="comments">
        
	
    <div id="vcomments" class="valine"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="/assets/valine.min.js"></script>

    <script>
        new Valine({
            av: AV,
            el: '#vcomments',
            appId: '5MzTXYXkt03k101j0PmSDN34-gzGzoHsz',
            appKey: 'iwjYgwno6qj3wtDVVSbe8nYQ',
            placeholder: '说点什么吧',
            notify: false,
            verify: true,
            avatar: 'mm',
            meta: 'nick,mail'.split(','),
            pageSize: '10',
            path: window.location.pathname,
            lang: 'zh-CN'.toLowerCase()
        })
    </script>


    </div>





                </main>
                
                    <aside id="article-toc" role="navigation" class="col-md-4">
    <div class="widget">
        <h3 class="title">文章目录</h3>
        
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#一-简介"><span class="toc-text">一. 简介</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#二-环境准备"><span class="toc-text">二. 环境准备</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-操作系统"><span class="toc-text">1. 操作系统</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-jdk"><span class="toc-text">2. jdk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-mysql"><span class="toc-text">3. mysql</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Python3"><span class="toc-text">4. Python3</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#检测环境变量，若无按如下配置："><span class="toc-text">检测环境变量，若无按如下配置：</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-打开环境变量配置"><span class="toc-text">1). 打开环境变量配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-配置PYTHON-HOME"><span class="toc-text">2). 配置PYTHON_HOME</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-配置Path"><span class="toc-text">3). 配置Path</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-IDE（PyCharm）"><span class="toc-text">5. IDE（PyCharm）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Scrapy"><span class="toc-text">6. Scrapy</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#安装方式一："><span class="toc-text">安装方式一：</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#安装wheel"><span class="toc-text">安装wheel</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#用途："><span class="toc-text">用途：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#安装命令："><span class="toc-text">安装命令：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#安装lxml"><span class="toc-text">安装lxml</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#用途：-1"><span class="toc-text">用途：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#安装命令：-1"><span class="toc-text">安装命令：</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#安装方式一-在线-："><span class="toc-text">安装方式一(在线)：</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#安装方式二（离线-："><span class="toc-text">安装方式二（离线)：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#安装zope-interface"><span class="toc-text">安装zope.interface</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#用途：-2"><span class="toc-text">用途：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#安装命令：-2"><span class="toc-text">安装命令：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#安装pyOpenSSL"><span class="toc-text">安装pyOpenSSL</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#用途：-3"><span class="toc-text">用途：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#安装命令：-3"><span class="toc-text">安装命令：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#安装Twisted"><span class="toc-text">安装Twisted</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#用途：-4"><span class="toc-text">用途：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#安装命令：-4"><span class="toc-text">安装命令：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#安装pywin32"><span class="toc-text">安装pywin32</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#用途：-5"><span class="toc-text">用途：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#安装命令：-5"><span class="toc-text">安装命令：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#安装Scrapy"><span class="toc-text">安装Scrapy</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#安装命令：-6"><span class="toc-text">安装命令：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#可能出现的问题"><span class="toc-text">可能出现的问题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#安装方式二：使用Anaconda工具，快速解决系列"><span class="toc-text">安装方式二：使用Anaconda工具，快速解决系列</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#验证安装是否成功"><span class="toc-text">验证安装是否成功</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#方法一："><span class="toc-text">方法一：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#方法二："><span class="toc-text">方法二：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#文件的功能"><span class="toc-text">文件的功能</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基本使用"><span class="toc-text">基本使用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#a-创建工程结构"><span class="toc-text">a.  创建工程结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#b-创建一个spider（爬虫文件）"><span class="toc-text">b.  创建一个spider（爬虫文件）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#c-运行"><span class="toc-text">c.  运行</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#三-项目构建"><span class="toc-text">三. 项目构建</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Mysql建库"><span class="toc-text">1. Mysql建库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-百度百科爬虫"><span class="toc-text">2. 百度百科爬虫</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#修改item-py"><span class="toc-text">修改item.py</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#修改-pipelines-py"><span class="toc-text">修改 pipelines.py</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#修改中间件-middlewares-py"><span class="toc-text">修改中间件 middlewares.py</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#修改-settings-py"><span class="toc-text">修改 settings.py</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#编写-baidu-baike-py"><span class="toc-text">编写 baidu_baike.py</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#运行项目"><span class="toc-text">运行项目</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-互动百科爬虫"><span class="toc-text">3. 互动百科爬虫</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#四-总结"><span class="toc-text">四. 总结</span></a></li></ol>
        
    </div>
</aside>

                
            </div>
        </div>
    </section>
    <footer class="main-footer">
    <div class="container">
        <div class="row">
        </div>
    </div>
</footer>

<a id="back-to-top" class="icon-btn hide">
	<i class="fa fa-chevron-up"></i>
</a>




    <div class="copyright">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="busuanzi">
    
</div>

            </div>
            <div class="col-sm-12">
                <span>Copyright &copy; 2019-2024&emsp;<a href="/" class="copyright-links" target="_blank" rel="nofollow">Ljjyy.com</a>
                </span> |
                <span>
                    <a href="/about/" class="copyright-links" target="_blank" rel="nofollow">关于我们</a>
                </span> |                
                <span>
                    <a href="/sitemap.xml" class="copyright-links" target="_blank" rel="nofollow">网站地图</a>
                </span> |
                <span>
                    <a href="/archives/" class="copyright-links" target="_blank" rel="nofollow">时间轴</a>
                </span>              
            </div>
        </div>
    </div>
</div>







<script src="/js/app.js?rev=@@hash"></script>

</body>
</html>